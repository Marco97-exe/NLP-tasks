GENERAL


- import libs
- set randomness
- download corpus and glove -> glove use pytorch text ? 
- put both in dataframe
- stats and data visualization
- build vocab for words in corpus -> id
- build vocab for words in gloves -> embeddings
- stats and data visualization
- preprocessing if needed
- save things , check if already saved, reload, ecc 
- build embedding matrix and handle OOV 
- build the model 
- train, validation, test split 


- download_and_unzip_dataset()





RIMETTERE LO 0 COME PAD IDX


PIPELINES:
    -Train Pipeline:
        -Inputs: (model, x_train, y_train, x_val, y_val, **training_info**)
            training_info -> model fit() argument information
        - History: model.fit(x_train, y_train, validation_data)
            Plots the History
        -Outputs: trained model
    
    -Predict Pipeline:
        -Inputs: (model, set_examples, prediction_info)
        -Predictions: model.predict(set_examples, **prediction_info)
        -Returns: predictions
    
    -Evaluation Pipeline:
        -Inputs: (predictions, y, metrics, metrics_names)
        -Metrics: metric_info={}
            matric_value: metric_i(y_pred=predictions, y_true=y)... forall metrics
            metric_info[metric_name]: metric_value
        -Outputs: metric_info 




SPECIFIC 
- encode df containing sentences in df of idxs ->tokenization
- implement decoder function to test that everything is ok 
- forse index 0 reserved 



CONDA ENV 

conda create --name nlp
conda install python=3.9
conda install -c conda-forge jupyterlab
conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch    // se avete la gpu
conda install pytorch torchvision torchaudio cpuonly -c pytorch             // se non avete la gpu
conda install pandas 
conda install -c anaconda gensim
