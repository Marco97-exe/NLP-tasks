{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Part-of Speech (POS) tagging as Sequence Labelling using RNNs_**\n",
    "\n",
    "**Authors**: Giacomo Berselli, Marco CucÃ¨, Riccardo De Matteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "from operator import itemgetter\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "# Fix data seed to achieve reproducible results\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current work directory: {}\".format(os.getcwd()))\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\") # directory containing the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenized dataframe\n",
    "\n",
    "The Dataset is the `dependency_treebank` Dataset from the `NLTK` data module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def download_and_unzip_dataset():\n",
    "    \"\"\"\n",
    "        Downloads and unzips the dataset for the assignment\n",
    "    \"\"\"\n",
    "    dataset_folder = os.path.join(data_folder, \"dependency_treebank\")\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        print('downloading and extracting dataset to :',dataset_folder)\n",
    "        with urlopen(url) as response:\n",
    "            zipfile = ZipFile(BytesIO(response.read()))\n",
    "            zipfile.extractall(path=data_folder)\n",
    "    else :\n",
    "        print(\"the dataset has been already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the dataset we encode it in a pandas dataframe and make some preprocessing, as follows:\n",
    "- Removing of the third column because useless.\n",
    "- Introduction of a boolean variable `to_lower`, which if set to `True` reduce all words to lower case (the results will be discussed in a next cell).\n",
    "- Since each document can contain multiple sentences, we decide to split documents into sentences. Having to carry out a sequence labelling task, the grammatical analysis for tag assignment takes into account only single sentences, while there is no correlation between POS tags of words in different sentences.\\\n",
    "In addition, this allows us to insert vectors with a very similar length into the dataloaders, reducing the padding size.\n",
    "\n",
    "Then, we create a pandas dataframe where each row represents a sentence and with the following columns:\n",
    "- `split`: `train` if sentence belongs to a document of range 1-100, `val` if 101-150 and `test` if 151-199.\n",
    "- `doc_id`: document's number containing that sequence.\n",
    "- `sentence_num`: number to identify the sentence in the document.\n",
    "- `words`: list of words inside the sentence.\n",
    "- `tags`: list of tags relative to the words of the sentence.\n",
    "- `num_tokens`: total number of words/tags inside the sentence.\n",
    "\n",
    "Finally, we return the new developed pandas dataframe `df_final` and the list of unique tags `unique_tags` and unique words `unique_words`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode dataset in pandas dataframe \n",
    "\n",
    "def encode_dataset(dataset_name: str, to_lower: bool) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "    Takes the dataset and encodes it in a pandas dataframe having six columns ['split', 'doc_id', 'sentence_num', 'words', 'tags', 'num_tokens']. Computes also unique tags set and unique words set and returns them with the dataframe.\n",
    "  \n",
    "  \"\"\"\n",
    "  print(\"Encoding dataset as pandas dataframe...\")\n",
    "\n",
    "  dataset_folder = os.path.join(data_folder,\"dependency_treebank\")\n",
    "  \n",
    "  dataframe_rows = []             #dataframe that will contain all the sentences in all the documents, each sentence as a list of word and a list of corresponding tags\n",
    "  unique_tags = set()\n",
    "  unique_words = set()\n",
    "\n",
    "  for doc in os.listdir(dataset_folder):\n",
    "    doc_num = int(doc[5:8])\n",
    "    doc_path = os.path.join(dataset_folder,doc)\n",
    "\n",
    "    with open(doc_path, mode='r', encoding='utf-8') as file:\n",
    "      df = pd.read_csv(file,sep='\\t',header=None,skip_blank_lines=False)\n",
    "      df.rename(columns={0:'word',1:\"TAG\",2:\"remove\"},inplace=True)\n",
    "      df.drop(\"remove\",axis=1,inplace=True)\n",
    "\n",
    "      if to_lower: df['word'] = df[\"word\"].str.lower() #set all words to lower case\n",
    "      \n",
    "      #create another column that indicate group by sentence \n",
    "      df[\"group_num\"] = df.isnull().all(axis=1).cumsum()\n",
    "      df.dropna(inplace=True)\n",
    "      df.reset_index(drop=True, inplace=True)\n",
    "      \n",
    "      unique_tags.update(df['TAG'].unique())     #save all the unique tags in a set \n",
    "      unique_words.update(df['word'].unique())   #save all the unique words in a set \n",
    "\n",
    "      #generate sentence list in a document \n",
    "      df_list = [df.iloc[rows] for _, rows in df.groupby('group_num').groups.items()]\n",
    "      for n,d in enumerate(df_list) :           #for each sentence create a row in the final dataframe\n",
    "          dataframe_row = {\n",
    "              \"split\" : 'train' if doc_num<=100 else ('val' if doc_num<=150  else 'test'),\n",
    "              \"doc_id\" : doc_num,\n",
    "              \"sentence_num\" : n,\n",
    "              \"words\": d['word'].tolist(),\n",
    "              \"tags\":  d['TAG'].tolist(),\n",
    "              \"num_tokens\": len(d['word'])\n",
    "          }\n",
    "          dataframe_rows.append(dataframe_row)\n",
    "\n",
    "  dataframe_path = os.path.join(data_folder, dataset_name)\n",
    "  df_final = pd.DataFrame(dataframe_rows)\n",
    "  df_final.to_csv(dataframe_path + \".csv\")                      #save as csv to inspect\n",
    "\n",
    "  print(\"Encoding completed!\")\n",
    "    \n",
    "  return  df_final, unique_tags, unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to build the dictionaries that will be used in the development of the embedding matrix and one hot encoding of the tags.\n",
    "\\\n",
    "\\\n",
    "The function `build_dict` takes the list of words and tags of the dataframe, and returns:\n",
    "- `word2int`: dictionary which associates each word with an integer.\n",
    "- `int2word`: dictionary which associates each integer with the relative word.\n",
    "- `tag2int`: dictionary which associates each tag with an integer.\n",
    "- `int2tag`: dictionary which associates each integer with the relative tag.\n",
    "\n",
    "In addition, we save these four dictionaries in the `dictionaries.pkl` file to load them if already available and make the process faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(words : list[str], tags : list[str]): \n",
    "    \"\"\"\n",
    "        Builds 4 dictionaries word2int, int2word, tag2int, int2tag and returns them\n",
    "    \"\"\"\n",
    "    \n",
    "    word2int = OrderedDict()\n",
    "    int2word = OrderedDict()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        word2int[word] = i+1\n",
    "        int2word[i+1] = word\n",
    "\n",
    "    tag2int = OrderedDict()\n",
    "    int2tag = OrderedDict()\n",
    "\n",
    "    for i, tag in enumerate(tags):\n",
    "        tag2int[tag] = i+1\n",
    "        int2tag[i+1] = tag\n",
    "    \n",
    "    print('saving dictionaries as pickle files')\n",
    "    pickle_files = [word2int,int2word,tag2int,int2tag]\n",
    "    files_path = os.path.join(data_folder,'dictionaries.pkl')\n",
    "    with open(files_path, 'wb') as f:\n",
    "        pickle.dump(pickle_files, f)\n",
    "\n",
    "    return word2int,int2word,tag2int,int2tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once builded the dictionaries, we use them in the function `build_tokenized_dataframe` which tokenize each word and tag of the dataframe and create a tokenized one. Then we add a new column called `split`, which identifies if a sentence belongs to the train, validation or test part, and another one called `num_tokens` storing the number of words in that sentence.\n",
    "\\\n",
    "\\\n",
    "In order to verify if the words and tags of the original dataset have been tokenized well, we create also the function `check_dataframe_tokenization`. In particular, it makes a comparison between the original words of the dependency treebank dataset and the ones of the new tokenized dataframe. Then, if everything is OK, we save the dataframe as a pickle file called `token_dataset.pkl`, again to load it if already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenized_dataframe(word2int: Dict, tag2int: Dict, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "        Given the dictionaries word2int, tag2int and the dataframe, creates a dataframe were every word and tag is represented by its token and returns it\n",
    "    \"\"\"\n",
    "    print('Initiating tokenization of words and tags in dataframe')\n",
    "    tokenized_rows = []\n",
    "    for words,tags in zip(df['words'],df['tags']):\n",
    "        tokenized_row = {'words_token':[word2int[word] for word in words ],'tags_token':[tag2int[tag] for tag in tags ]}\n",
    "        tokenized_rows.append(tokenized_row)\n",
    "    \n",
    "    tokenized_df = pd.DataFrame(tokenized_rows)\n",
    "\n",
    "    tokenized_df.insert(0,'split',df['split'])\n",
    "    tokenized_df.insert(1,'num_tokens',df['num_tokens'])\n",
    "\n",
    "    print('Tokenization completed')\n",
    "\n",
    "    return tokenized_df\n",
    "\n",
    "\n",
    "def check_dataframe_tokenization(tokenized_df, normal_df, int2word, int2tag) :\n",
    "    \"\"\"\n",
    "        Security method that checks if the tokenized dataframe will lead to the normal dataframe usind the dictionaries int2word and int2tag\n",
    "    \"\"\"\n",
    "    for n, (w_t, t_t) in enumerate(zip(tokenized_df['words_token'],tokenized_df['tags_token'])):\n",
    "        if not normal_df.loc[n,'words'] == [int2word[word_token] for word_token in w_t]:\n",
    "            print('words tokenization gone wrong') \n",
    "            return False\n",
    "        if not normal_df.loc[n,'tags'] == [int2tag[tag_token] for tag_token in t_t]:\n",
    "            print('tags tokenization gone wrong')\n",
    "            return False \n",
    "    \n",
    "    print('all right with dataset tokenization')\n",
    "    print('saving tokenized dataframe')\n",
    "    path = os.path.join(data_folder, \"token_dataset\")\n",
    "    tokenized_df.to_pickle(path+'.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we download the GloVe embeddings which we will use to build the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_glove_emb():   \n",
    "    \"\"\"\n",
    "        Download the glove embedding and returns it \n",
    "    \"\"\"\n",
    "    print('downloading glove embeddings ')        \n",
    "    embedding_dimension=300\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    emb_model = gloader.load(download_path)\n",
    "    \n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once downloaded the embeddings we can detect if there are some Out Of Vocabulary (OOV) words.\n",
    "\\\n",
    "\\\n",
    "A word is considered OOV if it is present in the dataframe used by the model but not in the GloVe embeddings. Therefore, we take each word from the `unique_words` list of the dataframe and search if it is present inside the embeddings, otherwise we add it to a list called `oov_words`.\n",
    "\\\n",
    "\\\n",
    "Next, the function prints as output the number of unique words in the dataframe, the percentage of OOV terms with respect to the total number of words, and an example of some words not present in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, unique_words: list[str], lower: bool):\n",
    "    \"\"\"\n",
    "        Given the embedding model and the unique words in the dataframe computes and prints the out-of-vocabulary words \n",
    "    \"\"\"\n",
    "    oov_words = []\n",
    "\n",
    "    if lower:\n",
    "        words = set([x.lower() for x in unique_words])\n",
    "    else: \n",
    "        words = unique_words\n",
    "\n",
    "    for word in words:\n",
    "        try: \n",
    "           embedding_model[word]\n",
    "        except:\n",
    "           oov_words.append(word) \n",
    "    \n",
    "    print(\"Total number of unique words in dataset:\",len(words))\n",
    "    print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(words))*100))\n",
    "    print(\"Some OOV terms:\",random.sample(oov_words,15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen next, the `dependency_treebank` dataset contains a total number of 11968 unique words. By using the GloVe embeddings for the construction of the embedding matrix, we obtain 3745 OOV words, the 31.29% of all words in the dataset.\\\n",
    "Below are printed some random examples of these words:\\\n",
    "`['Darrell', 'Series', 'Merc', 'Economic', 'Officials', 'Jaguar', 'Environment', 'twin-jet']`\\\n",
    "It is clear how most of them begin with a capital letter.\n",
    "\n",
    "For this reason we have introduced the boolean `to_lower` discussed previously.\n",
    "Once all words have been reduced to lower case, the OOV words within the dataframe become 676, the 6.18% of all words.\n",
    "\n",
    "We tested both configurations but we don't observe substantial difference in the results, so we have decided to set `to_lower = False`. This decision was taken because, by analyzing the words within the sentences and having to assign a tag to each of them, the presence of capital letters for proper nouns or words at the beginning of the sentences could help the network for a better evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Embedding matrix\n",
    "\n",
    "Now we can build our embedding matrix by passing to the function `build_embedding_matrix` the GloVe embeddings and the `word2int` dictionary, thus:\n",
    "- If a dataframe's word (identified by its relative integer) is already present as GloVe embedding we store it as it is in the embedding matrix.\n",
    "- Otherwise we add the OOV word to the matrix by using as embedding a random vector of size 300 (= GloVe embedding dimension) from a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value_distribution_glove(glove: gensim.models.keyedvectors.KeyedVectors):\n",
    "    \"\"\"\n",
    "        Computes maximum and minimum values of GloVe embedding and prints them\n",
    "    \"\"\"\n",
    "    max_v = np.max([(np.max(glove[i])) for i in range(len(glove))])\n",
    "    min_v = np.min([(np.min(glove[i])) for i in range(len(glove))])\n",
    "\n",
    "    print('Max value inside glove embeddings:',max_v)\n",
    "    print('Min value inside glove embeddings:',min_v)\n",
    "\n",
    "\n",
    "def build_embedding_matrix(emb_model: gensim.models.keyedvectors.KeyedVectors, word2int: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Given the embedding model and the dict. word2int. If there is the embedding for the word, we add it to the embedding_matrix. In negative case we put a list of random values.\n",
    "        Return the embedding matrix\n",
    "    \"\"\"\n",
    "    check_value_distribution_glove(emb_model)\n",
    "   \n",
    "    embedding_dimension = len(emb_model[0])                                                              \n",
    "    embedding_matrix = np.zeros((len(word2int)+1, embedding_dimension), dtype=np.float32)\n",
    "\n",
    "    for word, idx in word2int.items():\n",
    "        try:\n",
    "            embedding_vector = emb_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    \n",
    "    print('Saving emb matrix to pickle file')\n",
    "    path = os.path.join(data_folder, \"emb_matrix\")\n",
    "    np.save(path,embedding_matrix,allow_pickle=True)\n",
    "\n",
    "    print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check that the embedding matrix has been builded well, here we check that the integer associated to each word of the tokenized dataframe correspond with the index of the same word inside the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_id_corr(int2word : Dict[int,str],glove: gensim.models.keyedvectors.KeyedVectors, matrix, dataframe ):\n",
    "    \"\"\"\n",
    "        Checks whether the tokenized dataframe and the index of the embedding matrix correspond\n",
    "    \"\"\"\n",
    "    oov_words_ = []\n",
    "\n",
    "    for token_sentence in dataframe['words_token']:\n",
    "\n",
    "        for token in token_sentence:\n",
    "            emb1 = matrix[token]\n",
    "            word = int2word[token]\n",
    "            emb2 = None\n",
    "            try:\n",
    "                emb2 = glove[word]\n",
    "            except:\n",
    "                oov_words_.append(word)\n",
    "            if emb2 is not None:\n",
    "                assert(np.array_equal(emb1,emb2))\n",
    "\n",
    "    print('Double check OOV number:',len(set(oov_words_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the functions that we need to build our embedding matrix, so we can call them in order and wait few minutes until the process is completed.\n",
    "\\\n",
    "\\\n",
    "In order to download the GloVe embeddings only once (the process is really slow), we also check if the data folder `data` is already present in the working directory. In that case, all the functions defined earlier would not be recomputed because all the datasets are already present and everything works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_folder):\n",
    "    print('This is the first run! Data still not present')\n",
    "\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "    download_and_unzip_dataset()\n",
    "\n",
    "    df, unique_tags, unique_words = encode_dataset(\"dataset\", to_lower = False)\n",
    "\n",
    "    word2int,int2word,tag2int,int2tag = build_dict(unique_words,unique_tags)\n",
    "\n",
    "    tokenized_df = build_tokenized_dataframe(word2int,tag2int,df)\n",
    "\n",
    "    check_dataframe_tokenization(tokenized_df,df, int2word, int2tag)\n",
    "\n",
    "    glove_embeddings = download_glove_emb()\n",
    "\n",
    "    check_OOV_terms(glove_embeddings, unique_words,False)\n",
    "\n",
    "    embedding_matrix = build_embedding_matrix(glove_embeddings, word2int)\n",
    "    \n",
    "    check_id_corr(int2word,glove_embeddings,embedding_matrix,tokenized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whatever the case, now the folder `data` exist, so we call `load_data` in order to load the datasets and perform next the sequence labelling task.\\\n",
    "In practice we return:\n",
    "- `emb_matrix`: the embedding matrix.\n",
    "- `token_dataset`: the dataset tokenized with the integers in place of words and tags.\n",
    "- `word2int`: dictionary associating each word with its integer.\n",
    "- `int2word`: reverse of `word2int`.\n",
    "- `tag2int`: dictionary associating each tag with its integer.\n",
    "- `int2tag`: reverse of `tag2int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "        Loads the data \"emb_matrix, token_dataset, word2int, int2word, tag2int, int2tag \" and returns them\n",
    "    \"\"\"\n",
    "    emb_matrix_path = os.path.join(data_folder,'emb_matrix.npy')\n",
    "    token_dataset_path = os.path.join(data_folder,'token_dataset.pkl')\n",
    "    dictionaries_path = os.path.join(data_folder,'dictionaries.pkl')\n",
    "\n",
    "    if os.path.exists(emb_matrix_path) and os.path.exists(token_dataset_path):\n",
    "        print('loading embedding matrix')\n",
    "        emb_matrix = np.load(emb_matrix_path,allow_pickle=True)\n",
    "        print('loading tokenized dataset')\n",
    "        token_dataset = pd.read_pickle(token_dataset_path)\n",
    "        print('loading dictionaries')\n",
    "        with open(dictionaries_path, 'rb') as f:\n",
    "            word2int,int2word,tag2int,int2tag = pickle.load(f)\n",
    "        \n",
    "        print('all data loaded')\n",
    "    else:\n",
    "        print('searched data is not present in folder')\n",
    "        emb_matrix, token_dataset = None, None\n",
    "\n",
    "    return emb_matrix, token_dataset, word2int, int2word, tag2int, int2tag\n",
    "\n",
    "emb_matrix, token_dataset, word2int, int2word, tag2int, int2tag = load_data()\n",
    "\n",
    "token_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we need also to import all the libraries which we will use for the POS tagging task.\\\n",
    "In particular:\n",
    "- We have decided to use `pytorch` to build the RNNs.\n",
    "- The results of the train, val and test loop have been plotted using the `tensorboard` platform.\n",
    "- In order to analyze the errors and think about improvements, we took advantage of the `sklearn` tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytoch import \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.legacy.data import BucketIterator\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to take advantage of the Gradient Descent, we can divide the dataframe of sequences that we pass to the network in mini-batches, due to the dimensionality of it. The mini-batches are created using the `BucketIterator` package from `torchtext` which ensure in addition that each mini-batch is composed of sequences of nearly the same length, in order to add the minimum padding possible to each vector.\\\n",
    "Thus:\n",
    "- `DataframeDataset`: transform the tokenized dataframe into a dataset, because `BucketIterator` needs in input a dataset type.\n",
    "- `create_dataloaders`: divide the tokenized dataframe for training, validation and test. Then pass each of them to `DataframeDataset` and the outputs to `BucketIterator` which return the dataloaders.\n",
    "- `check_data_loaders`: count the maximum difference of length between the sentences of each mini-batch and print random examples to understand if everything works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataframeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "        self.X = dataframe['words_token']\n",
    "        self.y = dataframe['tags_token']\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return {'words': self.X[idx],'tags': self.y[idx]}\n",
    "\n",
    "def create_dataloaders(b_s : int):     #b_s = batch_size\n",
    "    \n",
    "    train_df = token_dataset[token_dataset['split'] == 'train'].reset_index(drop=True)\n",
    "    val_df = token_dataset[token_dataset['split'] == 'val'].reset_index(drop=True)\n",
    "    test_df = token_dataset[token_dataset['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = DataframeDataset(train_df)\n",
    "    val_dataset = DataframeDataset(val_df)\n",
    "    test_dataset = DataframeDataset(test_df)\n",
    "\n",
    "\n",
    "    # Group similar length text sequences together in batches.\n",
    "    train_dataloader,val_dataloader,test_dataloader = BucketIterator.splits((train_dataset,val_dataset,test_dataset),\n",
    "                                                        batch_sizes=(b_s,b_s,b_s), sort_key=lambda x: len(x['words']), \n",
    "                                                        repeat=True, sort=False, shuffle=True, sort_within_batch=True)\n",
    "    \n",
    "    return train_dataloader,val_dataloader,test_dataloader \n",
    "\n",
    "\n",
    "def check_data_loaders(train_dataloader,val_dataloader,test_dataloader):\n",
    "\n",
    "    for n,dataloader in enumerate((train_dataloader,val_dataloader,test_dataloader)):\n",
    "\n",
    "        dataloader.create_batches() # Create batches - needs to be called before each loop.\n",
    "\n",
    "        max_diff = -1\n",
    "        for batch in dataloader.batches:\n",
    "\n",
    "            min = np.min([len(example['words']) for example in batch])\n",
    "            max = np.max([len(example['words']) for example in batch])\n",
    "\n",
    "            diff = max - min\n",
    "\n",
    "            if diff > max_diff: max_diff = diff \n",
    "        \n",
    "        s = 'train' if n==0 else ('val' if n==1 else 'test')\n",
    "        \n",
    "        print('in',s+'_dataloader the maximum difference in number of tokens between two sentences in the same batch is:',max_diff)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    #print random sentence from train_dataloader\n",
    "    from operator import itemgetter\n",
    "    train_dataloader.create_batches()\n",
    "    for batch in train_dataloader.batches:\n",
    "        for example in batch:\n",
    "            print('random sentence from train_dataloader:')\n",
    "            print(itemgetter(*example['words'])(int2word))\n",
    "            print(itemgetter(*example['tags'])(int2tag))\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen later, the maximum length distance between two sentences in the batches of the train dataloader is 192 (witch `BATCH_SIZE = 8`), while for the validation set is 25 and the test set is 5.\n",
    "\\\n",
    "\\\n",
    "This huge difference is given by a single very long sentence within a document of the train set. We tried to remove it and compare the performances but we don't notice any improvement, so we decided to leave the dataframe unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Models\n",
    "\n",
    "Now it's time to build our custom model. The layers are as follows:\n",
    "- `self.embedding`: embedding layer, builded by passing to `create_emb_layer` the pretrained embedding matrix, used to create the layer.\n",
    "- `self.rnn`: RNN layer (by changing the value of `use_GRU` it can be used the LSTM or the GRU).\n",
    "- `self.middle_dense`: default value set to `None`, meaning that the model will have only one dense layer. To add another one, the boolean `double_dense` must be set to `True`.\n",
    "- `self.hidden2tag`: first (and perhaps only) dense layer of the network, encodes the outputs of the hidden layer in the final tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix: np.ndarray, pad_idx : int):\n",
    "    \"\"\"\n",
    "        creates adn returns the embedding layer\n",
    "    \"\"\"\n",
    "    matrix = torch.Tensor(weights_matrix)\n",
    "    _ , embedding_dim = matrix.shape \n",
    "    emb_layer = nn.Embedding.from_pretrained(matrix, freeze=True, padding_idx = pad_idx)\n",
    "    \n",
    "    return emb_layer, embedding_dim\n",
    "\n",
    "class custom_model(nn.Module):\n",
    "    \"\"\"\n",
    "        Class defining our models \n",
    "    \"\"\"\n",
    "    def __init__(self, emb_matrix : np.ndarray, hidden_dim: int, tag_output_dim: int, pad_idx: int, double_lstm : bool, double_dense : bool, use_GRU: bool) :\n",
    "        super().__init__()\n",
    "\n",
    "        if double_lstm + double_dense + use_GRU == 0: \n",
    "            self.name = 'naive'\n",
    "        else :\n",
    "            self.name = 'double_lstm' if double_lstm else ('double_dense' if double_dense else 'use_GRU') \n",
    "\n",
    "        self.embedding, embedding_dim = create_emb_layer(emb_matrix,pad_idx)\n",
    "\n",
    "        if use_GRU:\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first = True, num_layers = int(double_lstm)+1, bidirectional = True)\n",
    "        else :\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first = True, num_layers = int(double_lstm)+1, bidirectional = True)\n",
    "\n",
    "        self.middle_dense = None \n",
    "\n",
    "        if double_dense:\n",
    "            self.middle_dense = nn.Linear(hidden_dim*2,hidden_dim)\n",
    "            self.hidden2tag = nn.Linear(hidden_dim, tag_output_dim)\n",
    "        else :\n",
    "            self.hidden2tag = nn.Linear(hidden_dim * 2 , tag_output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        \n",
    "        embeds = self.embedding(sentences)\n",
    "        out, _  = self.rnn(embeds)\n",
    "\n",
    "        if self.middle_dense is not None :\n",
    "            out = self.middle_dense(out)\n",
    "\n",
    "        tag_space = self.hidden2tag(out)\n",
    "        \n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we need some 'helper' functions that we will use in the train and test pipeline, which are: \n",
    "- `init_weights`: initialize the weights and biases of the LSTM, Fully Connected and GRU (if `use_GRU = True`) layers, by forcing mean = 0 and standard deviation = 0.1.\n",
    "- `compute_class_weights`: computes weights to be passed to loss function for counteract imbalanced data. In particular, it will be given more weight to the loss function when the model predict wrongly a tag associated to words which are very few in the train dataset.\n",
    "- `get_to_be_masked_tags`: returns a `torch.Tensor` which contains the indexes of the punctuation's tags because no evaluation of punctuation in the final result.\n",
    "- `reshape_and_mask`: reshapes the tensors of the predicted and true tags (in order to be compliant with `nn.crossEntropyLoss`) and delete the punctuation tags.\n",
    "- `acc_and_f1`: compute accuracy and f1 score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELPER FUNCTIONS \n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "        Initiliaze weights of LSTM, GRU & FC\n",
    "    \"\"\"\n",
    "    for _, param in m.named_parameters():\n",
    "        if isinstance(m, nn.LSTM) or isinstance(m, nn.Linear) or isinstance(m, nn.GRU):\n",
    "            nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "\n",
    "def compute_class_weights():\n",
    "    \"\"\"\n",
    "        Compute weights to be passed to loss function for counteract imbalanced data \n",
    "    \"\"\"\n",
    "    y = token_dataset[token_dataset['split']=='train']['tags_token'].explode().to_numpy()\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    w = np.concatenate(([0],compute_class_weight('balanced',classes=classes,y=y)))\n",
    "    \n",
    "    return torch.Tensor(w)\n",
    "\n",
    "\n",
    "def get_to_be_masked_tags():\n",
    "    \"\"\"\n",
    "        Return a torch tensor which contains the indexes of the tags that we don't want to evaluate (punctuation)\n",
    "    \"\"\"\n",
    "    punctuation_tags = ['$', '``', '.', ',', '#', 'SYM', ':', \"''\",'-RRB-','-LRB-']\n",
    "    token_punctuations = [tag2int[tag] for tag in punctuation_tags]    \n",
    "\n",
    "    print('the indexes of punct tags:',token_punctuations) # int of punctuation's tokens\n",
    "    print([int2tag[token_int] for token_int in token_punctuations]) # TODO only for testing\n",
    "\n",
    "    return torch.LongTensor(token_punctuations+[0]) #0 is the pad token \n",
    "\n",
    "to_mask = get_to_be_masked_tags()\n",
    "\n",
    "\n",
    "def reshape_and_mask(predictions: torch.Tensor,targets: torch.LongTensor):\n",
    "    \"\"\"\n",
    "        Return two tensors : the predicted labels and the true labels, both removing unwanted classes and reshaped to be compliant with crossEntropyLoss\n",
    "    \"\"\"\n",
    "    max_preds = predictions.argmax(dim=1)   #get the index of maximum value of n-dim vector (n is the number of possible tags) to retrive model prediction \n",
    "    non_masked_elements = torch.isin(targets, to_mask, invert=True)   #create a mask removing unwanted tags \n",
    "    \n",
    "    return max_preds[non_masked_elements],targets[non_masked_elements]    #return tensors masked \n",
    "\n",
    "\n",
    "def acc_and_f1(y_pred: torch.LongTensor, y_true: torch.LongTensor):\n",
    "    \"\"\"\n",
    "        Return accuracy and f1-score of every example (word) in an epoch \n",
    "    \"\"\"\n",
    "    correct = y_pred.eq(y_true)\n",
    "    acc = correct.sum()/y_true.shape[0] \n",
    "\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    return acc,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train and Validation\n",
    "\n",
    "In this section, we finally define the pipelines used to train and evaluate our models. They will be used in the subsequent cells in order to evaluate different architectures while keeping hyperparameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: nn.Module, iterator : BucketIterator, optimizer: optim.Optimizer, criterion, pad_idx : int):\n",
    "    \"\"\"\n",
    "        Train loop that return the epoch loss, epoch accuracy and epoch f1 score\n",
    "    \"\"\"\n",
    "    batch_loss = 0\n",
    "    \n",
    "    tot_pred , tot_targ = torch.LongTensor(), torch.LongTensor()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    iterator.create_batches()\n",
    "\n",
    "    for batch_id, batch in enumerate(iterator.batches):\n",
    "\n",
    "        batch_X = [torch.LongTensor(example['words']) for example in batch]\n",
    "        batch_y = [torch.LongTensor(example['tags']) for example in batch]\n",
    "\n",
    "        padded_X = rnn.pad_sequence(batch_X, batch_first = True, padding_value = pad_idx)\n",
    "        padded_y = rnn.pad_sequence(batch_y, batch_first = True, padding_value = pad_idx)\n",
    "\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        optimizer.zero_grad()                #TODO forse non serve ne basta uno dei due \n",
    "\n",
    "        predictions = model(padded_X)\n",
    "        predictions = predictions.view(-1,predictions.shape[-1])\n",
    "        targets = padded_y.view(-1)\n",
    "\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        pred, targ = reshape_and_mask(predictions,targets)\n",
    "        tot_pred = torch.cat((tot_pred,pred))\n",
    "        tot_targ = torch.cat((tot_targ,targ))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "\n",
    "    epoch_loss = batch_loss/(batch_id+1)\n",
    "    epoch_acc, epoch_f1 = acc_and_f1(tot_pred,tot_targ)\n",
    "\n",
    "    return epoch_loss,epoch_acc,epoch_f1\n",
    "\n",
    "\n",
    "def eval_loop(model: nn.Module, iterator: BucketIterator, criterion, pad_idx):\n",
    "    \"\"\"\n",
    "        Evaluation loop returns (without training) the epoch loss, epoch accuracy and epoch f1 score\n",
    "    \"\"\"\n",
    "    batch_loss = 0\n",
    "    \n",
    "    tot_pred , tot_targ = torch.LongTensor(), torch.LongTensor()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    iterator.create_batches()\n",
    "\n",
    "    with torch.no_grad(): #without gradient since it is evaluation loop\n",
    "    \n",
    "        for batch_id, batch in enumerate(iterator.batches):\n",
    "\n",
    "            batch_X = [torch.LongTensor(example['words']) for example in batch]\n",
    "            batch_y = [torch.LongTensor(example['tags']) for example in batch]\n",
    "\n",
    "            padded_X = rnn.pad_sequence(batch_X, batch_first = True, padding_value = pad_idx)\n",
    "            padded_y = rnn.pad_sequence(batch_y, batch_first = True, padding_value = pad_idx)\n",
    "            \n",
    "            predictions = model(padded_X)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            targets = padded_y.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, targets)\n",
    "            \n",
    "            pred, targ = reshape_and_mask(predictions,targets)\n",
    "            tot_pred = torch.cat((tot_pred,pred))\n",
    "            tot_targ = torch.cat((tot_targ,targ))\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "\n",
    "    epoch_loss = batch_loss/(batch_id+1)\n",
    "    epoch_acc, epoch_f1 = acc_and_f1(tot_pred,tot_targ)\n",
    "\n",
    "    return epoch_loss,epoch_acc,epoch_f1,tot_pred,tot_targ\n",
    "\n",
    "\n",
    "def train_and_eval(architectures, param, train_dataloader,eval_dataloader):\n",
    "    \"\"\"\n",
    "        Runs the train and eval loop and keeps track of all the information of the model [best f1 score, predicted tags, corresponding tags, model, epoch]. Returns those infos\n",
    "    \"\"\"\n",
    "    models_info = {}\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = param['pad_idx'], weight = compute_class_weights() if param['use_class_weights'] else None)\n",
    "\n",
    "    for architecture in architectures:\n",
    "\n",
    "        #log on tensorboard \n",
    "        run_id = datetime.today().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "        writer = SummaryWriter('tensorboard/'+run_id)\n",
    "\n",
    "        #unpack parameters for model creation \n",
    "        double_lstm, double_dense, use_gru = architecture.double_lstm, architecture.double_dense, architecture.use_gru\n",
    "\n",
    "        model = custom_model(emb_matrix,param['hidden_state'],param['output_dim'],param['pad_idx'],double_lstm,double_dense,use_gru)      #create model \n",
    "\n",
    "        if param['use_init_weights'] :  model.apply(init_weights)           #weight inizialization \n",
    "\n",
    "        if param['optimizer'] == 'Adam': \n",
    "            optimizer = optim.Adam(model.parameters()) \n",
    "        elif param['optimizer'] == 'SGD': \n",
    "            optimizer = optim.SGD(model.parameters(),lr=param['lr'])\n",
    "        else : \n",
    "            raise Exception('optimizer must be either SGD or Adam')\n",
    "        \n",
    "        models_info[model.name] = {'best_f1':-1}\n",
    "\n",
    "        for epoch in range(param['n_epochs']): #epoch loop\n",
    "\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_epoch_loss, train_epoch_acc, train_epoch_f1 = train_loop(model, train_dataloader, optimizer, criterion, param['pad_idx']) #train\n",
    "            eval_epoch_loss, eval_epoch_acc, eval_epoch_f1, tot_pred, tot_targ = eval_loop(model, eval_dataloader, criterion, param['pad_idx']) #eval\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            tot_epoch_time = end_time-start_time          \n",
    "            \n",
    "            if eval_epoch_f1 > models_info[model.name]['best_f1']: #updates infos if current f1 is better then the previous one \n",
    "\n",
    "                models_info[model.name]['best_f1'] = eval_epoch_f1\n",
    "                models_info[model.name]['tot_pred'], models_info[model.name]['tot_targ'] =  tot_pred, tot_targ\n",
    "                models_info[model.name]['model'] = model \n",
    "                models_info[model.name]['epoch'] = epoch+1\n",
    "    \n",
    "\n",
    "            #log accuracy data on tensorboard  \n",
    "            writer.add_scalar(\"Train Acc\", train_epoch_acc, epoch+1)\n",
    "            writer.add_scalar(\"Val Acc\", eval_epoch_acc, epoch+1)\n",
    "            writer.add_scalar(\"Train F1\", train_epoch_f1, epoch+1)\n",
    "            writer.add_scalar(\"Val F1\", eval_epoch_f1, epoch+1)\n",
    "\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {tot_epoch_time:.4f}')\n",
    "            print(f'\\tTrain Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f}')\n",
    "            print(f'\\t Val. Loss: {eval_epoch_loss:.3f} | Eval. Acc: {eval_epoch_acc*100:.2f}% | Eval. F1: {eval_epoch_f1:.2f}')\n",
    "\n",
    "        writer.close()\n",
    "    \n",
    "    return models_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an hyperparameter tuning phase, here are the ones that we are using to evaluate each architecture:\n",
    "- `BATCH_SIZE = 8`: we have experimented with the sizes a little bit, but we don't want batches to be neither too small, to avoid noise, nor too big, because of the sizes of the dataset's splits. Since we don't see big differences for a batch size range between 4 and 16, we decided to set `BATCH_SIZE = 8`.\n",
    "- `LR = 0.4`: we have obtained the best results using `Adam`, which set automatically the initial learning rate. In case someone wants to use another one, like `SGD`, an `LR = 0.4` is optimal.\n",
    "- `HIDDEN_STATE = 128`: it is well known that setting an high dimension of the hidden layers would ensure to the networks the ability to represents better the informations between the states, with respect to a small hidden dimension. For this reason we tried three different dimensions: 64, 128 and 256. The results of the comparison are quite similar but confirm the assumption, and we see that an `HIDDEN_STATE = 128` already represents well the data.\n",
    "- `N_EPOCHS = 10`: with a small batch size, `N_EPOCHS = 10` seems enough to obtain good results and avoid overfitting.\n",
    "- `USE_INIT_WEIGHTS = False`: as already described, we have inserted a weight initialization for the LSTM and FC layers. Unfortunately it seems that initialize weights with `mean = 0` and `std = 0.1` doesn't actually help the convergence of the gradient. On the contrary, by setting `USE_INIT_WEIGHTS = False` allows to obtain an increase on the F1 score of 1-2% points.\n",
    "- `USE_CLASS_WEIGHTS = False`: if set to `True` the model would call the function `compute_class_weights` which we have defined in order to balance weights of cross entropy loss.Nevertheless, to train our model we decided to set it to `False`, the reasons will be explained in the error analysis section.\n",
    "- `OPTIMIZER = 'Adam'`: we have tried many different types of gradient descent optimizers, such as: `SGD`, `SGD` with `momentum`, `RMS-prop` and `Adam`. For this particular task, `Adam` seems to be the best choice, althought choosing the best configurations of hyperparameters for `SGD` with `momentum` could achieve comparable performances.\n",
    "\n",
    "The architectures to evaluate must be placed in the `architectures` namedtuple. In particular, we analyze:\n",
    "- `naive`: one embedding layer, one LSTM layer and one FC layer.\n",
    "- `two_lstm`: one embedding layer, two LSTM layers and one FC layer.\n",
    "- `two_dense`: one embedding layer, one LSTM layer and two FC layers.\n",
    "- `gru`: one embedding layer, one GRU layer and one FC layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS, HYPERPARAMETERS AND OBJECTS \n",
    "\n",
    "#param\n",
    "PAD_IDX = 0                     # pad index\n",
    "OUTPUT_DIM = len(tag2int)+1     # 0 is reserved \n",
    "CHECK_LOADERS = True            # check the data loaders \n",
    "\n",
    "#hyperparameters\n",
    "BATCH_SIZE = 8                  # number of sentences in each mini-batch\n",
    "LR = 0.4                        # learning rate (not used if OPTIMIZER = 'Adam') \n",
    "HIDDEN_STATE = 128              # number of nodes in hidden state\n",
    "N_EPOCHS = 10                   # number of epochs\n",
    "USE_INIT_WEIGHTS = False        # if `True` initialize weights and biases of LSTM, GRU and FC layers \n",
    "USE_CLASS_WEIGHTS = False       # if `True` balance weights of cross entropy loss \n",
    "OPTIMIZER = 'Adam'              # choose Gradient Descent optimizer to use ('Adam' or 'SGD')\n",
    "\n",
    "param = {'lr':LR,\n",
    "        'hidden_state':HIDDEN_STATE,\n",
    "        'n_epochs':N_EPOCHS,\n",
    "        'use_init_weights':USE_INIT_WEIGHTS,\n",
    "        'use_class_weights':USE_CLASS_WEIGHTS,\n",
    "        'optimizer':OPTIMIZER,\n",
    "        'pad_idx':PAD_IDX,\n",
    "        'output_dim':OUTPUT_DIM}\n",
    "\n",
    "train_dataloader,val_dataloader,test_dataloader = create_dataloaders(BATCH_SIZE)\n",
    "\n",
    "if CHECK_LOADERS : check_data_loaders(train_dataloader,val_dataloader,test_dataloader)\n",
    "\n",
    "architecture = namedtuple('architecture',['double_lstm','double_dense','use_gru'])\n",
    "\n",
    "naive = architecture(False,False,False)\n",
    "two_lstm = architecture(True,False,False)\n",
    "two_dense = architecture(False,True,False)\n",
    "gru = architecture(False,False,True)\n",
    "\n",
    "architectures = [naive,two_lstm,two_dense,gru]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_info = train_and_eval(architectures,param,train_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMMENTA RISULTATI SOPRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the models trained, we can take the two with the best f1 score and save into `two_best_models`, in order to test these ones further with the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_models = dict(sorted(models_info.items(), key=lambda item: item[1]['best_f1'], reverse=True))    #order the models in models_info based on their f1_score \n",
    "\n",
    "two_best_models = OrderedDict()      #store in a dictionary the two best models \n",
    "\n",
    "for n,(k,v) in enumerate(ordered_models.items()):\n",
    "    two_best_models[k] = v\n",
    "    if n==1 : break            #take only the best two \n",
    "\n",
    "best_model_name = next(iter(two_best_models))    #we need the very best model to evaluate its errors \n",
    "best_model_info = two_best_models[best_model_name]\n",
    "\n",
    "best_f1, pred, targ = best_model_info['best_f1'],best_model_info['tot_pred'],best_model_info['tot_targ']\n",
    "\n",
    "print('the best model is :',best_model_name,'with',best_f1,'f1-score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMMENTA F1 SOPRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Error analysis on Validation Set\n",
    "\n",
    "Now that we have trained all the models we can perform a simple error analysis about the results of the validation set, in order to understand the errors and think about possible improvements.\\\n",
    "In particular, we have developed the follow functions:\n",
    "- `build_classification_report`: create classification report showing the precision, recall, f1-score and support for each tag and the final accuracy.\n",
    "- `build_confusion_matrix`: create a confusion matrix.\n",
    "- `build_errors_dictionary`: create a dictionary which associates to each tag a list of the wrong tags that are predicted instead of the true one and the total number of times it happens for the validation set.\n",
    "- `get_tag_distribution`: show the number of occurrences of each tag in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def build_classification_report(targ,pred,unique_tags):\n",
    "    \"\"\"\n",
    "        Generate classification report \n",
    "    \"\"\"\n",
    "    report = classification_report(targ,pred,zero_division=0,output_dict=False,target_names=unique_tags)\n",
    "    print(report)\n",
    "\n",
    "\n",
    "def build_confusion_matrix(targ,pred,unique_tags):\n",
    "    \"\"\"\n",
    "        Build confusion matrix and returns it as a dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    cf_matrix = confusion_matrix(targ, pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix, index = unique_tags, columns = unique_tags)\n",
    "\n",
    "    #plot the confusion matrix as heatmap \n",
    "    plt.figure(figsize = (40,32))\n",
    "    sn.heatmap(df_cm, annot=True, cmap=\"Blues\", linewidths= 0.05, linecolor='white')\n",
    "\n",
    "    return df_cm\n",
    "\n",
    "def build_errors_dictionary(df_cm):\n",
    "    \"\"\"\n",
    "        Create a dictionary which contains, for each true tag a list of wrong predicted tags and the number of wrong predictions\n",
    "    \"\"\"\n",
    "\n",
    "    errors = {}\n",
    "    for true_tag,row in df_cm.iterrows():\n",
    "\n",
    "        tag_errors = []\n",
    "        for pred_tag, occurrences in row.iteritems() : \n",
    "            if not pred_tag==true_tag and occurrences!=0 : \n",
    "                tag_errors.append((pred_tag,occurrences))\n",
    "        \n",
    "        tag_errors.sort(key = itemgetter(1),reverse=True)\n",
    "\n",
    "        if tag_errors:     \n",
    "            errors[true_tag] = tag_errors\n",
    "\n",
    "    errors = dict(sorted(errors.items(), key = lambda item : item[1][0][1],reverse=True))    #sort the dictionary in order to have the more wrongly classified tags on top \n",
    "\n",
    "    #pretty print \n",
    "    print('true_TAG --> (pred_TAG, n_times)\\n')\n",
    "    for k,v in errors.items():\n",
    "        print(k,'-->',*v)\n",
    "\n",
    "def get_tag_distribution():\n",
    "    \"\"\"\n",
    "        Count occurrences of each TAG in the train set \n",
    "    \"\"\"\n",
    "    tag_frequency = {}\n",
    "    df_temp = token_dataset[token_dataset['split'] == 'train']\n",
    "    for _ ,row in df_temp.iterrows():\n",
    "        for key in row['tags_token']:\n",
    "            tag_frequency[int2tag[key]] = tag_frequency.get(int2tag[key],0) + 1\n",
    "\n",
    "    return dict(sorted(tag_frequency.items(), key=lambda item: item[1], reverse = True))\n",
    "\n",
    "def plot_tag_occur(occ: Dict):\n",
    "    \"\"\"\n",
    "        Creates and plots an image showing the distribution of each unique tag inside the dataset\n",
    "    \"\"\"\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_ylabel('num occurrences in train dataset')\n",
    "    ax.set_xticks(range(len(occ.keys())))\n",
    "    ax.bar(occ.keys(), occ.values())\n",
    "\n",
    "    plt.title(\"Tags distribution in train dataset\")\n",
    "    fig.set_figwidth(25)\n",
    "    fig.set_figheight(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell computes the number of unique tags which have been predicted well or wrongly (removing as usual the punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [int2tag[i] for i in set(targ.tolist() + pred.tolist())]                           #tags as string instead of indexes, for every unique tag in pred or tag \n",
    "print('the number of unique tags in either the predictions or targets is: ',len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training the model for few epochs, it can be seen how the number of unique tags (true + predicted) are greater than 37 (number of total tags without punctuation). This behaviour is justified by the fact that the model performs really bad and predict wrongly as punctuation some words which aren't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_classification_report(targ,pred,tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report is great to understand what are the main tags that we predict wrongly most of the times.\\\n",
    "By looking at the plot, we can see two bad behaviours:\n",
    "- Some tags have high precision but low recall, so we predict them well but we also produce a lot of false positives.\n",
    "- Other tags instead have low precision and low recall so we are completely wrong in predicting them.\n",
    "\n",
    "By making a comparison between these two types of tags, it can be seen how both have a low support, so there are not many of them in the train dataframe.\n",
    "\\\n",
    "\\\n",
    "For this reason, we have developed the function `compute_class_weights` which add weights to the cross entropy loss. It implements the homonymous one of `sklearn` which give more weight to the prediction errors of class underrepresented in the train dataframe, and less to the ones overrepressented. \n",
    "\\\n",
    "\\\n",
    "Although the function does what it is supposed to do, effectively reducing prediction errors for the less represented classes, the result is now however also an increase in prediction errors among the more represented classes, unfortunately offsetting the positive effects. Therefore, we decided not to use the function during the training of the models.\n",
    "\\\n",
    "\\\n",
    "A possible improvement that would certainly lead to better results would be to increase the weight of prediction errors on underrepresented classes by balancing it with that of more represented classes, so as not to create substantial differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = build_confusion_matrix(targ, pred, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is useful to understand which tags we get wrong the most and with which we predict them wrongly.\\\n",
    "In order to have a more understandable result of the errors we can use the `build_errors_dictionary` function that we have previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_errors_dictionary(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set a threshold of nearly 70 recurrent fail predictions, and by looking at the results above these are the ones most relevant:\n",
    "- `NN-JJ`: prediction of nouns as adjectives (because of a lot of examples of `NN` but few of `JJ`) and viceversa.\n",
    "- `NN-NNP`: prediction of singular nouns as singular proper nouns (because very similar).\n",
    "- `JJ-NNP`: prediction of adjectives as singular proper nouns (because of a lot of examples of `NNP` but few of `JJ`) and viceversa.\n",
    "- `NNPS-NNP`: prediction of plural proper nouns as singular proper nouns (because of few examples of `NNPS` but a lot of `NNP`).\n",
    "- `NNS-NN`: prediction of plural nouns as singular nouns (because of a lot of examples of `NN` but few of `NNS`) and viceversa.\n",
    "\n",
    "On the other hand, it's important to take a look also to fail predictions which occur rarely, because the cause is often that they are present only in few sequences, such as:\n",
    "- `LS-CD`: few times wrong prediction of cardinal numbers as list item marker (because of few examples of `LS` but a lot of `CD`).\n",
    "- `FW`: fail in predicting foreign words (because of few examples).\n",
    "- `UH`: fail in predicting interjections (because of few examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand which are the most frequent tags within the dataset and which are less frequent, we can plot the distribution of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dis = get_tag_distribution()\n",
    "\n",
    "plot_tag_occur(tag_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot reconfirms what has been previously stated, so that there are classes highly represented and others with almost zero examples. The main errors will be made between similar classes but with a big difference of representation in the dataset and on classes not very represented in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Test of two best models\n",
    "\n",
    "After evaluating the mistakes made on the validation set and improving the models accordingly, it is now time to evaluate the two best models using the sentences of the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX, weight = compute_class_weights() if USE_CLASS_WEIGHTS else None)\n",
    "\n",
    "test_pred, test_targ = None, None        #store the predicted labels and the true labels for the best model on test set \n",
    " \n",
    "for model_name,model_info in two_best_models.items():\n",
    "    model = model_info['model']\n",
    "    loss, acc, f1, tot_pred, tot_targ = eval_loop(model,test_dataloader,criterion,PAD_IDX)\n",
    "\n",
    "    print(model_name,f': Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}% | Test F1: {f1:.2f}')\n",
    "\n",
    "    if model_name == best_model_name:\n",
    "        test_pred, test_targ = tot_pred, tot_targ\n",
    "\n",
    "tags = [int2tag[i] for i in set(test_targ.tolist() + test_pred.tolist())]                           #tags as string instead of indexes, for every unique tag in pred or tag \n",
    "print('\\nthe number of unique tags in either the predictions or targets is: ',len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_classification_report(test_targ,test_pred,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = build_confusion_matrix(test_targ, test_pred, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_errors_dictionary(df_cm)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b10856870a6af5fec4ffddd4d7318a6f2add2c9f3b4bd7caecf75cea33b7bd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
