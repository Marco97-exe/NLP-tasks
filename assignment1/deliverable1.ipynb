{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Part-of Speech (POS) tagging as Sequence Labelling using RNNs_**\n",
    "\n",
    "**Authors**: Giacomo Martelli, Marco CucÃ¨, Riccardo De Matteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "from operator import itemgetter\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "# Fix data seed to achieve reproducible results\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current work directory: {}\".format(os.getcwd())) #print the current working directory \n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\") # directory containing the notebook\n",
    "\n",
    "if not os.path.exists(data_folder):   #create folder where all data will be stored \n",
    "    os.makedirs(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preparation\n",
    "\n",
    "Here we download the `dependency_treebank` dataset from the `NLTK` data module and we store it unzipped in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def download_and_unzip_dataset():\n",
    "    \"\"\"\n",
    "        Downloads and unzips the dataset for the assignment\n",
    "    \"\"\"\n",
    "    dataset_folder = os.path.join(data_folder, \"dependency_treebank\")\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        print('downloading and extracting dataset to :',dataset_folder)\n",
    "        with urlopen(url) as response:\n",
    "            zipfile = ZipFile(BytesIO(response.read()))\n",
    "            zipfile.extractall(path=data_folder)\n",
    "    else :\n",
    "        print(\"the dataset has been already downloaded\")\n",
    "\n",
    "download_and_unzip_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the dataset we encode it as a pandas dataframe and make some preprocessing, as follows:\n",
    "- Remove of the third column because useless.\n",
    "- Introduction of a boolean variable `to_lower`, which if set to `True` put all words to lower case. \n",
    "- Since each document can contain multiple sentences, we decided to split documents into sentences. Having to carry out a sequence labelling task, the grammatical analysis for tag assignment should take into account only single sentences, while there is no direct correlation between POS tags of words in different sentences.\\\n",
    "In addition, this will allow us to group sentences with similar length in batches, while documents are more variable in size, thus reducing the padding size.\n",
    "\n",
    "After that a new pandas dataframe is created, where each row represents a sentence. The dataframe has the following columns:\n",
    "- `split`: `train` if sentence belongs to a document of range 1-100, `val` if 101-150 and `test` if 151-199.\n",
    "- `doc_id`: document's number containing that sequence.\n",
    "- `sentence_num`: number to identify the sentence in the document.\n",
    "- `words`: list of words inside the sentence.\n",
    "- `tags`: list of tags relative to the words of the sentence.\n",
    "- `num_tokens`: total number of words/tags inside the sentence (for testing purposes). \n",
    "\n",
    "Finally, we return the newly generated pandas dataframe `df_final` and the list of unique tags `unique_tags` and unique words `unique_words` in the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode dataset in pandas dataframe \n",
    "\n",
    "def encode_dataset(dataset_name: str, to_lower: bool) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "    Takes the dataset and encodes it in a pandas dataframe having six columns ['split', 'doc_id', 'sentence_num', 'words', 'tags', 'num_tokens']. Computes also unique tags set and unique words set and returns them with the dataframe.\n",
    "  \n",
    "  \"\"\"\n",
    "  print(\"Encoding dataset as pandas dataframe...\")\n",
    "\n",
    "  dataset_folder = os.path.join(data_folder,\"dependency_treebank\")\n",
    "  \n",
    "  dataframe_rows = []             #dataframe that will contain all the sentences in all the documents, each sentence as a list of word and a list of corresponding tags\n",
    "  unique_tags = set()             \n",
    "  unique_words = set()\n",
    "\n",
    "  for doc in os.listdir(dataset_folder):\n",
    "    doc_num = int(doc[5:8])\n",
    "    doc_path = os.path.join(dataset_folder,doc)\n",
    "\n",
    "    with open(doc_path, mode='r', encoding='utf-8') as file:\n",
    "      df = pd.read_csv(file,sep='\\t',header=None,skip_blank_lines=False)\n",
    "      df.rename(columns={0:'word',1:\"TAG\",2:\"remove\"},inplace=True)\n",
    "      df.drop(\"remove\",axis=1,inplace=True)\n",
    "\n",
    "      if to_lower: df['word'] = df[\"word\"].str.lower() #set all words to lower case\n",
    "      \n",
    "      #create another column that indicate the group id by sentence \n",
    "      df[\"group_num\"] = df.isnull().all(axis=1).cumsum()\n",
    "      df.dropna(inplace=True)\n",
    "      df.reset_index(drop=True, inplace=True)\n",
    "      \n",
    "      unique_tags.update(df['TAG'].unique())     #save all the unique tags in a set \n",
    "      unique_words.update(df['word'].unique())   #save all the unique words in a set \n",
    "\n",
    "      #generate sentence list in a document \n",
    "      df_list = [df.iloc[rows] for _, rows in df.groupby('group_num').groups.items()]\n",
    "      for n,d in enumerate(df_list) :           #for each sentence create a row in the final dataframe\n",
    "          dataframe_row = {\n",
    "              \"split\" : 'train' if doc_num<=100 else ('val' if doc_num<=150  else 'test'),\n",
    "              \"doc_id\" : doc_num,\n",
    "              \"sentence_num\" : n,\n",
    "              \"words\": d['word'].tolist(),\n",
    "              \"tags\":  d['TAG'].tolist(),\n",
    "              \"num_tokens\": len(d['word'])\n",
    "          }\n",
    "          dataframe_rows.append(dataframe_row)\n",
    "\n",
    "  dataframe_path = os.path.join(data_folder, dataset_name)\n",
    "  df_final = pd.DataFrame(dataframe_rows)\n",
    "  df_final.to_csv(dataframe_path + \".csv\")                      #save as csv to inspect\n",
    "\n",
    "  print(\"Encoding completed!\")\n",
    "    \n",
    "  return  df_final, unique_tags, unique_words\n",
    "\n",
    "df, unique_tags, unique_words = encode_dataset(\"dataset\", to_lower = False)\n",
    "\n",
    "print('Some words from the dataset:', random.choices(list(unique_words),k=15))\n",
    "print('Some tags from the dataset:', random.choices(list(unique_tags),k=15))\n",
    "\n",
    "print('\\nencoded dataframe:')\n",
    "df.head(10)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to build the dictionaries that will be used for the generation of the embedding matrix and one hot encoding (done by the loss function) of the tags.\n",
    "\\\n",
    "\\\n",
    "The function `build_dict` takes the list of words and tags of the dataframe, and returns:\n",
    "- `word2int`: dictionary which associates each word with an integer.\n",
    "- `int2word`: dictionary which associates each integer with the relative word.\n",
    "- `tag2int`: dictionary which associates each tag with an integer.\n",
    "- `int2tag`: dictionary which associates each integer with the relative tag.\n",
    "\n",
    "In addition, we save these four dictionaries in the `dictionaries.pkl` file to load them if already available and make the process faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = os.path.join(data_folder,'dictionaries.pkl') #path where dictionaries will be saved \n",
    "\n",
    "def build_dict(words : list[str], tags : list[str]): \n",
    "    \"\"\"\n",
    "        Builds 4 dictionaries word2int, int2word, tag2int, int2tag and returns them\n",
    "    \"\"\"\n",
    "    \n",
    "    word2int = OrderedDict()\n",
    "    int2word = OrderedDict()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        word2int[word] = i+1           #plus 1 since the 0 will be used as tag token \n",
    "        int2word[i+1] = word\n",
    "\n",
    "    tag2int = OrderedDict()\n",
    "    int2tag = OrderedDict()\n",
    "\n",
    "    for i, tag in enumerate(tags):\n",
    "        tag2int[tag] = i+1\n",
    "        int2tag[i+1] = tag\n",
    "    \n",
    "    print('saving dictionaries as pickle files')\n",
    "    pickle_files = [word2int,int2word,tag2int,int2tag]\n",
    "    \n",
    "    with open(dict_path, 'wb') as f:\n",
    "        pickle.dump(pickle_files, f)\n",
    "\n",
    "    return word2int,int2word,tag2int,int2tag\n",
    "\n",
    "word2int,int2word,tag2int,int2tag = build_dict(unique_words,unique_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dictionaries have been built, we use them in the function `build_tokenized_dataframe` which tokenizes each word and tag of the dataframe by substituting to them the correspondent index found in the dictionary. Then we add a new column called `split`, which identifies if a sentence belongs to the train, validation or test part, and another one called `num_tokens` storing the number of words in that sentence.\n",
    "\\\n",
    "\\\n",
    "The function `check_dataframe_tokenization` is used in order to verify if the words and tags of the original dataset have been tokenized correctly. In particular, it makes a comparison between the original words of the dependency treebank dataset and the ones of the new tokenized dataframe. Then, if everything is OK, we save the dataframe as a pickle file called `token_dataset.pkl`, again to load it afterwards, if already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df_path = os.path.join(data_folder, \"token_dataset.pkl\") #tokenized dataframe path\n",
    "\n",
    "def build_tokenized_dataframe(word2int: Dict, tag2int: Dict, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "        Given the dictionaries word2int, tag2int and the dataframe, creates a dataframe were every word and tag is represented by its token and returns it\n",
    "    \"\"\"\n",
    "    print('Initiating tokenization of words and tags in dataframe')\n",
    "    tokenized_rows = []\n",
    "    for words,tags in zip(df['words'],df['tags']):\n",
    "        tokenized_row = {'words_token':[word2int[word] for word in words ],'tags_token':[tag2int[tag] for tag in tags ]}\n",
    "        tokenized_rows.append(tokenized_row)\n",
    "    \n",
    "    tokenized_df = pd.DataFrame(tokenized_rows)\n",
    "\n",
    "    tokenized_df.insert(0,'split',df['split'])\n",
    "    tokenized_df.insert(1,'num_tokens',df['num_tokens'])\n",
    "\n",
    "    print('Tokenization completed')\n",
    "\n",
    "    return tokenized_df\n",
    "\n",
    "\n",
    "def check_dataframe_tokenization(tokenized_df, normal_df, int2word, int2tag) :\n",
    "    \"\"\"\n",
    "       Checks if the tokenized dataframe will lead to the normal dataframe usind the dictionaries int2word and int2tag\n",
    "    \"\"\"\n",
    "    for n, (w_t, t_t) in enumerate(zip(tokenized_df['words_token'],tokenized_df['tags_token'])):\n",
    "        if not normal_df.loc[n,'words'] == [int2word[word_token] for word_token in w_t]:\n",
    "            print('words tokenization gone wrong') \n",
    "            return False\n",
    "        if not normal_df.loc[n,'tags'] == [int2tag[tag_token] for tag_token in t_t]:\n",
    "            print('tags tokenization gone wrong')\n",
    "            return False \n",
    "    \n",
    "    print('\\nAll right with dataset tokenization')\n",
    "    print('Saving tokenized dataframe')\n",
    "    \n",
    "    tokenized_df.to_pickle(token_df_path)\n",
    "\n",
    "\n",
    "tokenized_df = build_tokenized_dataframe(word2int,tag2int,df)\n",
    "check_dataframe_tokenization(tokenized_df,df, int2word, int2tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at the tokenized dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we will download the GloVe embeddings which will then be used to construct the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = os.path.join(data_folder, \"token_dataset.pkl\") #tokenized dataframe path\n",
    "\n",
    "def download_glove_emb():   \n",
    "    \"\"\"\n",
    "        Download the glove embedding and returns it \n",
    "    \"\"\"\n",
    "    print('downloading glove embeddings ')        \n",
    "    embedding_dimension=300\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    emb_model = gloader.load(download_path)\n",
    "    \n",
    "    return emb_model\n",
    "\n",
    "glove_embeddings = download_glove_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the glove embeddings, we can check if there are some Out Of Vocabulary (OOV) words.\n",
    "\\\n",
    "\\\n",
    "A word is considered OOV if it is present in our dataset but not in the GloVe embeddings. Therefore, we take each word from the `unique_words` list of the dataframe and look if it is present inside GloVe, otherwise we add it to a list called `oov_words`.\n",
    "\\\n",
    "\\\n",
    "Next, the function prints as output the number of unique words in the dataframe, the percentage of OOV terms with respect to the total number of words, and an example of some words not present in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, unique_words: list[str], lower: bool):\n",
    "    \"\"\"\n",
    "        Given the embedding model and the unique words in the dataframe computes and prints the out-of-vocabulary words \n",
    "    \"\"\"\n",
    "    oov_words = []\n",
    "\n",
    "    if lower:\n",
    "        words = set([x.lower() for x in unique_words])\n",
    "    else: \n",
    "        words = unique_words\n",
    "\n",
    "    for word in words:\n",
    "        try: \n",
    "           embedding_model[word]\n",
    "        except:\n",
    "           oov_words.append(word) \n",
    "    \n",
    "    print(\"Total number of unique words in dataset:\",len(words))\n",
    "    print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(words))*100))\n",
    "    print(\"Some OOV terms:\",random.sample(oov_words,15))\n",
    "\n",
    "check_OOV_terms(glove_embeddings, unique_words,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dependency_treebank` dataset contains a total number of 11968 unique words. By using the GloVe embeddings for our embedding matrix, we obtain 3745 OOV words, that is the 31.29% of all words in our dataset which is surprisingly high.\n",
    "\n",
    "Here is a random example of these words:\\\n",
    "`['Darrell', 'Series', 'Merc', 'Economic', 'Officials', 'Jaguar', 'Environment', 'twin-jet']`\\\n",
    "It is clear how many of them begin with a capital letter.\n",
    "\n",
    "For this reason we have introduced the possibility to put every word in the dataset to lowercase, as discussed previously.\n",
    "Once all words have been reduced to lower case, the OOV words within the dataframe become 676, the 6.18% of all words; much less than before.\n",
    "\n",
    "We tested both configurations but we didn't observe any improvement with having less OOV, so we have decided to set `to_lower = False`. This decision was taken because, having to assign a tag to each word within a sentence, the presence of capital letters could actually help the network in correctly labeling that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Embedding matrix\n",
    "\n",
    "We can finally build our embedding matrix by passing to the function `build_embedding_matrix` the GloVe embeddings and the `word2int` dictionary, thus:\n",
    "- If a dataframe's word (identified by its relative integer) is already present as GloVe embedding we store it as it is in the embedding matrix.\n",
    "- Otherwise we assign as embedding at the index of the corresponding OOV word, a random vector of size 300 (= GloVe embedding dimension) from a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to inspect values inside the glove embeddings. Return the max and min value inside vectors \n",
    "def check_value_distribution_glove(glove: gensim.models.keyedvectors.KeyedVectors):\n",
    "    \"\"\"\n",
    "        Computes maximum and minimum values of GloVe embedding and prints them\n",
    "    \"\"\"\n",
    "    max_v = np.max([(np.max(glove[i])) for i in range(len(glove))])\n",
    "    min_v = np.min([(np.min(glove[i])) for i in range(len(glove))])\n",
    "\n",
    "    print('Max value inside glove embeddings:',max_v)\n",
    "    print('Min value inside glove embeddings:',min_v)\n",
    "\n",
    "\n",
    "def build_embedding_matrix(emb_model: gensim.models.keyedvectors.KeyedVectors, word2int: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Given the embedding model and the dict. word2int. If there is the embedding for the word, we add it to the embedding_matrix. In negative case we put a list of random values.\n",
    "        Return the embedding matrix\n",
    "    \"\"\"\n",
    "    check_value_distribution_glove(emb_model)\n",
    "   \n",
    "    embedding_dimension = len(emb_model[0]) #how many numbers each emb vector is composed of                                                           \n",
    "    embedding_matrix = np.zeros((len(word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n",
    "\n",
    "    for word, idx in word2int.items():\n",
    "        try:\n",
    "            embedding_vector = emb_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector     #assign the retrived or the generated vector to the corresponding index \n",
    "    \n",
    "    print('Saving embedding matrix')\n",
    "    path = os.path.join(data_folder, \"emb_matrix\")\n",
    "    np.save(path,embedding_matrix,allow_pickle=True)\n",
    "\n",
    "    print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = build_embedding_matrix(glove_embeddings, word2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's have a look at the first few rows of the freshly created embedding matrix, to get a sense of it. As we can see the very first row is full of zeros since it is the 'fake embedding' of the padding token which will never be used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(embedding_matrix).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be completely sure that the embedding matrix has been built correctly, we check that the embedding vector associated with an index in the embedding matrix is the same as the one retrieved from glove by passing to it the word to which that index correspond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_id_corr(int2word : Dict[int,str],glove: gensim.models.keyedvectors.KeyedVectors, matrix, dataframe ):\n",
    "    \"\"\"\n",
    "        Checks whether the tokenized dataframe and the index of the embedding matrix correspond\n",
    "    \"\"\"\n",
    "    oov_words_ = []\n",
    "\n",
    "    for token_sentence in dataframe['words_token']:\n",
    "\n",
    "        for token in token_sentence:\n",
    "            emb1 = matrix[token]\n",
    "            word = int2word[token]\n",
    "            emb2 = None\n",
    "            try:\n",
    "                emb2 = glove[word]\n",
    "            except:\n",
    "                oov_words_.append(word)\n",
    "            if emb2 is not None:\n",
    "                assert(np.array_equal(emb1,emb2))\n",
    "\n",
    "    print('Double check OOV number:',len(set(oov_words_)))\n",
    "\n",
    "check_id_corr(int2word,glove_embeddings,embedding_matrix,tokenized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no error has been thrown, we can safely proceed with the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be the first run or not. Do not run the above cells if this is not the first time you do it, in order to avoid downloading the GloVe embeddings more than once (the process is really slow).\\\n",
    "Whatever the case, now the folder `data` exists, so we call `load_data` in order to load all the data that we need to perform the next steps.\n",
    "\n",
    "In practice we need:\n",
    "- `emb_matrix`: the embedding matrix.\n",
    "- `token_dataset`: the dataset tokenized with the integers in place of words and tags.\n",
    "- `word2int`: dictionary associating each word with its integer.\n",
    "- `int2word`: reverse of `word2int`.\n",
    "- `tag2int`: dictionary associating each tag with its integer.\n",
    "- `int2tag`: reverse of `tag2int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding matrix\n",
      "Loading tokenized dataset\n",
      "Loading dictionaries\n",
      "All data loaded\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "        Loads the data \"emb_matrix, token_dataset, word2int, int2word, tag2int, int2tag \" and returns them\n",
    "    \"\"\"\n",
    "    emb_matrix_path = os.path.join(data_folder,'emb_matrix.npy')\n",
    "    token_dataset_path = os.path.join(data_folder,'token_dataset.pkl')\n",
    "    dictionaries_path = os.path.join(data_folder,'dictionaries.pkl')\n",
    "\n",
    "    if os.path.exists(emb_matrix_path) and os.path.exists(token_dataset_path):\n",
    "        print('Loading embedding matrix')\n",
    "        emb_matrix = np.load(emb_matrix_path,allow_pickle=True)\n",
    "        print('Loading tokenized dataset')\n",
    "        token_dataset = pd.read_pickle(token_dataset_path)\n",
    "        print('Loading dictionaries')\n",
    "        with open(dictionaries_path, 'rb') as f:\n",
    "            word2int,int2word,tag2int,int2tag = pickle.load(f)\n",
    "        \n",
    "        print('All data loaded')\n",
    "    else:\n",
    "        print('What you are looking for is not present in the folder')\n",
    "        emb_matrix, token_dataset = None, None\n",
    "\n",
    "    return emb_matrix, token_dataset, word2int, int2word, tag2int, int2tag\n",
    "\n",
    "emb_matrix, token_dataset, word2int, int2word, tag2int, int2tag = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start we also need to import all the libraries which we will use for the POS tagging task.\\\n",
    "In particular:\n",
    "- We have decided to use `pytorch` as deep learning framework.\n",
    "- The results of the train, val and test loop have been plotted using the `tensorboard` platform.\n",
    "- In order to analyze the errors and reason on improvements, we took advantage of the `sklearn` tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytoch imports\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.legacy.data import BucketIterator\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#scikit-learn imports \n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order generate mini-batches to be passed to the network for each split we leveraged a `torchtext` utility, such as `BucketIterator`. It ensures that each mini-batch is composed of sequences of nearly the same length (depending on the chosen batch size), in order to add the minimum padding possible to each Tensor.\\\n",
    "In order to do so, we needed to create a Pytorch Dataset since this is what is requested by the BucketIterator.\n",
    "\n",
    "Thus:\n",
    "- `DataframeDataset`: transforms the tokenized dataframe into a dataset, because `BucketIterator` needs in input a `Dataset` type.\n",
    "- `create_dataloaders`: divides the tokenized dataframe for training, validation and test. A `DataframeDataset` for each split is created and passed to `BucketIterator` which returns the dataloaders.\n",
    "- `check_data_loaders`: counts the maximum difference in length between the sentences of each mini-batch and prints random examples to understand if everything is working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataframeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "        self.X = dataframe['words_token']      #select only the column of tokenized words \n",
    "        self.y = dataframe['tags_token']       #select only the column of tokenized tags \n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return {'words': self.X[idx],'tags': self.y[idx]}\n",
    "\n",
    "def create_dataloaders(b_s : int):     #b_s = batch_size\n",
    "    \n",
    "    train_df = token_dataset[token_dataset['split'] == 'train'].reset_index(drop=True)      \n",
    "    val_df = token_dataset[token_dataset['split'] == 'val'].reset_index(drop=True)\n",
    "    test_df = token_dataset[token_dataset['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "    #create DataframeDataset objects for each split \n",
    "    train_dataset = DataframeDataset(train_df)\n",
    "    val_dataset = DataframeDataset(val_df)\n",
    "    test_dataset = DataframeDataset(test_df)\n",
    "\n",
    "\n",
    "    # Group similar length text sequences together in batches and return an iterator for each split.\n",
    "    train_dataloader,val_dataloader,test_dataloader = BucketIterator.splits((train_dataset,val_dataset,test_dataset),\n",
    "                                                        batch_sizes=(b_s,b_s,b_s), sort_key=lambda x: len(x['words']), \n",
    "                                                        repeat=True, sort=False, shuffle=True, sort_within_batch=True)\n",
    "    \n",
    "    return train_dataloader,val_dataloader,test_dataloader \n",
    "\n",
    "#this function is used to inspect the iterators. \n",
    "def check_data_loaders(train_dataloader,val_dataloader,test_dataloader):\n",
    "\n",
    "    for n,dataloader in enumerate((train_dataloader,val_dataloader,test_dataloader)):\n",
    "\n",
    "        dataloader.create_batches() # Create batches - needs to be called before each loop.\n",
    "\n",
    "        diff_list = []\n",
    "        for batch in dataloader.batches:\n",
    "\n",
    "            min = np.min([len(example['words']) for example in batch])     #minimum lenght of a Tensor (sentence) in a batch \n",
    "            max = np.max([len(example['words']) for example in batch])     #maximum lenght of a Tensor (sentence) in a batch \n",
    "\n",
    "            diff = max - min   #biggest difference\n",
    "\n",
    "            diff_list.append(diff)\n",
    "        \n",
    "        diff_list.sort(reverse=True)\n",
    "        \n",
    "        s = 'train' if n==0 else ('val' if n==1 else 'test')\n",
    "        print('In',s+'_dataloader the list of maximum differences between sentences for each batch is: \\n',diff_list)\n",
    "        print('In',s+'_dataloader the two biggest difference in number of tokens between two sentences in the same batch is:', diff_list[0],',', diff_list[1])\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    #print random sentence from train_dataloader\n",
    "    from operator import itemgetter\n",
    "    train_dataloader.create_batches()\n",
    "    for batch in train_dataloader.batches:\n",
    "        for example in batch:\n",
    "            print('random sentence from train_dataloader:')\n",
    "            print(itemgetter(*example['words'])(int2word))\n",
    "            print(itemgetter(*example['tags'])(int2tag))\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train_dataloader the list of maximum differences between sentences for each batch is: \n",
      " [204, 56, 6, 5, 4, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "In train_dataloader the two biggest difference in number of tokens between two sentences in the same batch is: 204 , 56\n",
      "In val_dataloader the list of maximum differences between sentences for each batch is: \n",
      " [21, 6, 3, 3, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "In val_dataloader the two biggest difference in number of tokens between two sentences in the same batch is: 21 , 6\n",
      "In test_dataloader the list of maximum differences between sentences for each batch is: \n",
      " [10, 6, 5, 4, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "In test_dataloader the two biggest difference in number of tokens between two sentences in the same batch is: 10 , 6\n",
      "\n",
      "\n",
      "random sentence from train_dataloader:\n",
      "('Now', ',', '13', 'years', 'later', ',', 'Mr.', 'Lane', 'has', 'revived', 'his', 'Artist', 'in', 'a', 'full-length', 'movie', 'called', '``', 'Sidewalk', 'Stories', ',', \"''\", 'a', 'poignant', 'piece', 'of', 'work', 'about', 'a', 'modern-day', 'tramp', '.')\n",
      "('RB', ',', 'CD', 'NNS', 'RBR', ',', 'NNP', 'NNP', 'VBZ', 'VBN', 'PRP$', 'NNP', 'IN', 'DT', 'JJ', 'NN', 'VBN', '``', 'NNP', 'NNP', ',', \"''\", 'DT', 'JJ', 'NN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.')\n"
     ]
    }
   ],
   "source": [
    "tr, vl, ts = create_dataloaders(16)\n",
    "check_data_loaders(tr,vl,ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the maximum length distance between two sentences in each batch is very little on average (depending on the chosen batch size) and for many batches even zero.\\\n",
    "We still observe a batch with a large maximum difference betweeen two sentences that belongs to it. \\\n",
    "This huge difference is given by a single very long sentence within a document of the train set. We tried to remove it and compare the performances but we didn't notice any improvement, so we decided to leave the dataframe unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model definition\n",
    "\n",
    "It's now time to build our custom model. We defined a modular class which, depending on the passed parameters, allows us to perform experiments with different architectures.\n",
    "\n",
    "- `self.embedding`: embedding layer, constructed by passing to `create_emb_layer` the pretrained embedding matrix. This layer is not trainable. \n",
    "- `self.rnn`: RNN layer (by changing the value of `use_GRU`, the LSTM or the GRU is used).\n",
    "- `self.middle_dense`: default value set to `None`, meaning that the model will have only one dense layer. To add another one, the boolean `double_dense` must be set to `True`.\n",
    "- `self.hidden2tag`: last (and perhaps only) dense layer of the network. Takes in input a vector for each word in a sentence and returns for each of them a vector whose length is the number of the token that we want to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix: np.ndarray, pad_idx : int):\n",
    "    \"\"\"\n",
    "        creates adn returns the embedding layer\n",
    "    \"\"\"\n",
    "    matrix = torch.Tensor(weights_matrix)   #the embedding matrix \n",
    "    _ , embedding_dim = matrix.shape \n",
    "    emb_layer = nn.Embedding.from_pretrained(matrix, freeze=True, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable \n",
    "    \n",
    "    return emb_layer, embedding_dim\n",
    "\n",
    "class custom_model(nn.Module):\n",
    "    \"\"\"\n",
    "        Class defining our models \n",
    "    \"\"\"\n",
    "    def __init__(self, emb_matrix : np.ndarray, hidden_dim: int, tag_output_dim: int, pad_idx: int, double_lstm : bool, double_dense : bool, use_GRU: bool) :\n",
    "        super().__init__()\n",
    "\n",
    "        #define the name of the specific architecture \n",
    "        if double_lstm + double_dense + use_GRU == 0: \n",
    "            self.name = 'naive'\n",
    "        else :\n",
    "            self.name = 'double_lstm' if double_lstm else ('double_dense' if double_dense else 'use_GRU') \n",
    "\n",
    "        self.embedding, embedding_dim = create_emb_layer(emb_matrix,pad_idx)   #instantiate the embedding layer \n",
    "\n",
    "        if use_GRU:\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first = True, num_layers = int(double_lstm)+1, bidirectional = True)\n",
    "        else :\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first = True, num_layers = int(double_lstm)+1, bidirectional = True)\n",
    "\n",
    "        self.middle_dense = None    #an optional layer that lays between the rnn layer and the last dense one\n",
    "\n",
    "        if double_dense:\n",
    "            self.middle_dense = nn.Linear(hidden_dim*2,hidden_dim)\n",
    "            self.hidden2tag = nn.Linear(hidden_dim, tag_output_dim)\n",
    "        else :\n",
    "            self.hidden2tag = nn.Linear(hidden_dim * 2 , tag_output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, sentences, sentences_lengths):\n",
    "        \n",
    "        embeds = self.embedding(sentences)\n",
    "        packed_embeds = pack_padded_sequence(embeds, sentences_lengths, batch_first=True, enforce_sorted=False)   #pack the sentences batch so that no unnecessary computation is performed for padding tokens\n",
    "\n",
    "        packed_out, _  = self.rnn(packed_embeds)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)     #pad the sequence back since it has to be passed to the dense layer \n",
    "\n",
    "        if self.middle_dense is not None :\n",
    "            out = self.middle_dense(out)\n",
    "\n",
    "        tag_space = self.hidden2tag(out)\n",
    "        \n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we define some 'helper' functions that will be used in the train and test pipeline. Those simple functions are: \n",
    "- `init_weights`: initialize the weights and biases of the LSTM, Fully Connected and GRU (if `use_GRU = True`) layers, by enforcing mean = 0 and standard deviation = 0.1.\n",
    "- `compute_class_weights`: computes weights to be used in the loss function to counteract imbalanced data. In particular, more weight will be given to the loss when the model predict wrongly a tag associated to words which are very few in the train dataset.\n",
    "- `get_to_be_masked_tags`: returns a `torch.Tensor` which contains the indexes of the punctuation's and symbols' tags to ignore them in the evaluation of the model.\n",
    "- `reshape_and_mask`: reshapes the tensors of the predicted and true labels (in order to be compliant with `nn.crossEntropyLoss`) and mask out the punctuation indexes.\n",
    "- `acc_and_f1`: compute accuracy and f1 score based on list of true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the indexes of punct tags: [11, 40, 32, 8, 20, 13, 29, 31, 34, 28]\n",
      "['$', '``', '.', ',', '#', 'SYM', ':', \"''\", '-RRB-', '-LRB-']\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    \"\"\"\n",
    "        Initiliaze weights of LSTM, GRU & FC\n",
    "    \"\"\"\n",
    "    for _, param in m.named_parameters():\n",
    "        if isinstance(m, nn.LSTM) or isinstance(m, nn.Linear) or isinstance(m, nn.GRU):\n",
    "            nn.init.normal_(param.data, mean = 0, std = 0.1)   #initialize weights with normal distribution\n",
    "\n",
    "def compute_class_weights():\n",
    "    \"\"\"\n",
    "        Compute weights to be passed to loss function to counteract imbalanced data \n",
    "    \"\"\"\n",
    "    y = token_dataset[token_dataset['split']=='train']['tags_token'].explode().to_numpy()   #put all the tags in the train dataset in a list \n",
    "    classes = np.unique(y)   #create a list of only unique tags \n",
    "\n",
    "    w = np.concatenate(([0],compute_class_weight('balanced',classes=classes,y=y)))    #use compute_class_weigth method from scikit-learn and add the 0 weight for the padding token \n",
    "    \n",
    "    return torch.Tensor(w)\n",
    "\n",
    "\n",
    "def get_to_be_masked_tags():\n",
    "    \"\"\"\n",
    "        Return a torch tensor which contains the indexes of the tags that we don't want to evaluate (punctuation)\n",
    "    \"\"\"\n",
    "    punctuation_tags = ['$', '``', '.', ',', '#', 'SYM', ':', \"''\",'-RRB-','-LRB-']   #tags to be masked \n",
    "    token_punctuations = [tag2int[tag] for tag in punctuation_tags]  #indexes of tags to be masked \n",
    "\n",
    "    print('the indexes of punct tags:',token_punctuations) # int of punctuation's tokens\n",
    "    print([int2tag[token_int] for token_int in token_punctuations]) \n",
    "\n",
    "    return torch.LongTensor(token_punctuations+[0]) #0 is the pad token \n",
    "\n",
    "to_mask = get_to_be_masked_tags()\n",
    "\n",
    "\n",
    "def reshape_and_mask(predictions: torch.Tensor, targets: torch.LongTensor):\n",
    "    \"\"\"\n",
    "        Return two tensors : the predicted labels and the true labels, both after removing unwanted classes and reshaped to be compliant with crossEntropyLoss\n",
    "    \"\"\"\n",
    "    max_preds = predictions.argmax(dim=1)   #get the index of maximum value of n-dim vector (n is the number of possible tags) to retrive model prediction \n",
    "    non_masked_elements = torch.isin(targets, to_mask, invert=True)   #create a boolean mask removing unwanted tags \n",
    "    \n",
    "    return max_preds[non_masked_elements],targets[non_masked_elements]    #return tensors masked \n",
    "\n",
    "\n",
    "def acc_and_f1(y_pred: torch.LongTensor, y_true: torch.LongTensor):\n",
    "    \"\"\"\n",
    "        Return accuracy and f1-score of every example (word) in an epoch \n",
    "    \"\"\"\n",
    "    correct = y_pred.eq(y_true)          \n",
    "    acc = correct.sum()/y_true.shape[0] \n",
    "\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    return acc,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the tags that we don't want to take into account for model evaluation scores, along with their corresponding indexes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train and Validation\n",
    "\n",
    "In this section, we finally define the pipelines used to train and evaluate our models. They will be used in the subsequent cells in order to evaluate different architectures while keeping the hyperparameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: nn.Module, iterator : BucketIterator, optimizer: optim.Optimizer, criterion, pad_idx : int):\n",
    "    \"\"\" Args:\n",
    "         - model: the sequence pos tagger model istantiated with fixed hyperparameters.\n",
    "         - iterator: dataloader for passing data to the network in batches \n",
    "         - optimizer: gradient descent optimizer for backward pass \n",
    "         - criterion: loss function \n",
    "         - pad_idx: index of the padding token \n",
    "        \n",
    "        Return:\n",
    "         - epoch_loss: the average loss computed as mean of the loss for each minibatch.\n",
    "         - epoch accuracy: accuracy for an epoch that doesn't take into account the punctuation marks and symbols.\n",
    "         - epoch_f1: f1 score for an epoch that doesn't take into account the punctuation marks and symbols.    \n",
    "    \"\"\"\n",
    "\n",
    "    batch_loss = 0\n",
    "    \n",
    "    tot_pred , tot_targ = torch.LongTensor(), torch.LongTensor()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    iterator.create_batches()    #generate batches from dataloader \n",
    "\n",
    "    for batch_id, batch in enumerate(iterator.batches):\n",
    "\n",
    "        batch_X = [torch.LongTensor(example['words']) for example in batch]    #list of tensors of words token for each sentence in a batch \n",
    "        batch_y = [torch.LongTensor(example['tags']) for example in batch]     #list of tensors of tags token for each sentence in a batch \n",
    "\n",
    "        batch_X_lenghts = [len(example['words']) for example in batch]         #lenght of each sentence before padding \n",
    "\n",
    "        #pad the sentences to create a fixed size batch \n",
    "        padded_X = rnn.pad_sequence(batch_X, batch_first = True, padding_value = pad_idx)\n",
    "        padded_y = rnn.pad_sequence(batch_y, batch_first = True, padding_value = pad_idx)\n",
    "\n",
    "        #zero the gradients \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        optimizer.zero_grad()            \n",
    "\n",
    "        predictions = model(padded_X, batch_X_lenghts)   #compute predictions \n",
    "\n",
    "        #reshape all predictions and targets for a batch \n",
    "        predictions = predictions.view(-1,predictions.shape[-1])    \n",
    "        targets = padded_y.view(-1)\n",
    "\n",
    "        loss = criterion(predictions, targets)         #compute the loss \n",
    "\n",
    "        pred, targ = reshape_and_mask(predictions,targets)\n",
    "        tot_pred = torch.cat((tot_pred,pred))          #concatenate the new tensors with the one computed in previous steps\n",
    "        tot_targ = torch.cat((tot_targ,targ))\n",
    "\n",
    "        #backward pass \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "\n",
    "    epoch_loss = batch_loss/(batch_id+1)\n",
    "    epoch_acc, epoch_f1 = acc_and_f1(tot_pred,tot_targ)\n",
    "\n",
    "    return epoch_loss,epoch_acc,epoch_f1\n",
    "\n",
    "\n",
    "def eval_loop(model: nn.Module, iterator: BucketIterator, criterion, pad_idx: int):\n",
    "    \"\"\" Args:\n",
    "         - model: the sequence pos tagger model istantiated with fixed hyperparameters.\n",
    "         - iterator: dataloader for passing data to the network in batches \n",
    "         - criterion: loss function \n",
    "         - pad_idx: index of the padding token \n",
    "        \n",
    "        Return:\n",
    "         - epoch_loss: the average loss computed as mean of the loss for each minibatch.\n",
    "         - epoch accuracy: accuracy for an epoch that doesn't take into account the punctuation marks and symbols.\n",
    "         - epoch_f1: f1 score for an epoch that doesn't take into account the punctuation marks and symbols.\n",
    "         - tot_pred: tensor containing the predicted labels (tag in numeric format) associated to each word received as input.\n",
    "         - tot_labels: tensor containing in order the true labels (tag in numeric format) associated to each word received as input.  \n",
    "    \"\"\"\n",
    "     \n",
    "    batch_loss = 0\n",
    "    \n",
    "    tot_pred , tot_targ = torch.LongTensor(), torch.LongTensor()\n",
    "    \n",
    "    model.eval()   #model in eval mode \n",
    "    \n",
    "    iterator.create_batches()\n",
    "\n",
    "    with torch.no_grad(): #without computing gradients since it is evaluation loop\n",
    "    \n",
    "        for batch_id, batch in enumerate(iterator.batches):\n",
    "\n",
    "            batch_X = [torch.LongTensor(example['words']) for example in batch]\n",
    "            batch_y = [torch.LongTensor(example['tags']) for example in batch]\n",
    "\n",
    "            batch_X_lenghts = [len(example['words']) for example in batch]\n",
    "\n",
    "            padded_X = rnn.pad_sequence(batch_X, batch_first = True, padding_value = pad_idx)\n",
    "            padded_y = rnn.pad_sequence(batch_y, batch_first = True, padding_value = pad_idx)\n",
    "            \n",
    "            predictions = model(padded_X,batch_X_lenghts)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            targets = padded_y.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, targets)\n",
    "            \n",
    "            pred, targ = reshape_and_mask(predictions,targets)\n",
    "            tot_pred = torch.cat((tot_pred,pred))\n",
    "            tot_targ = torch.cat((tot_targ,targ))\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "\n",
    "    epoch_loss = batch_loss/(batch_id+1)\n",
    "    epoch_acc, epoch_f1 = acc_and_f1(tot_pred,tot_targ)\n",
    "\n",
    "    return epoch_loss,epoch_acc,epoch_f1,tot_pred,tot_targ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the train and eval loop we can combine them. Those two phases will be alternated for each epoch in order to see the progresses made by our model.\\\n",
    "Here we also instantiate the optimizer, the loss criterion and the model itself with the parameters that we specify.\\\n",
    "The `train_and_eval` function takes as parameter the `Architectures` list which contains the different model architectures that we want to try and that will be trained and evaluated in turn over the entire dataset. A dictionary containing the results for each of these architectures is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(architectures, param, train_dataloader,eval_dataloader):\n",
    "    \"\"\"\n",
    "        Runs the train and eval loop and keeps track of all the information of the model [best f1 score, predicted tags, corresponding tags, model, epoch]. Returns those infos\n",
    "    \"\"\"\n",
    "    models_info = {}\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = param['pad_idx'], weight = compute_class_weights() if param['use_class_weights'] else None)  #instantiate the loss function possibly with weight \n",
    "\n",
    "    for architecture in architectures:     #for each specified architecture do : \n",
    "\n",
    "        #log on tensorboard \n",
    "        run_id = datetime.today().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "        writer = SummaryWriter('tensorboard/'+run_id)\n",
    "\n",
    "        #unpack parameters for model creation \n",
    "        double_lstm, double_dense, use_gru = architecture.double_lstm, architecture.double_dense, architecture.use_gru\n",
    "\n",
    "        model = custom_model(emb_matrix,param['hidden_state'],param['output_dim'],param['pad_idx'],double_lstm,double_dense,use_gru)      #build model \n",
    "\n",
    "        if param['use_init_weights'] :  model.apply(init_weights)           #weight inizialization \n",
    "\n",
    "        #init optimizer \n",
    "        if param['optimizer'] == 'Adam': \n",
    "            optimizer = optim.Adam(model.parameters()) \n",
    "        elif param['optimizer'] == 'SGD': \n",
    "            optimizer = optim.SGD(model.parameters(),lr=param['lr'])\n",
    "        else : \n",
    "            raise Exception('optimizer must be either SGD or Adam')\n",
    "        \n",
    "        models_info[model.name] = {'best_f1':-1}\n",
    "\n",
    "        print('\\ntrain and eval loops for architecture:',model.name)\n",
    "\n",
    "        for epoch in range(param['n_epochs']): #epoch loop\n",
    "\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_epoch_loss, train_epoch_acc, train_epoch_f1 = train_loop(model, train_dataloader, optimizer, criterion, param['pad_idx']) #train\n",
    "            eval_epoch_loss, eval_epoch_acc, eval_epoch_f1, tot_pred, tot_targ = eval_loop(model, eval_dataloader, criterion, param['pad_idx']) #eval\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            tot_epoch_time = end_time-start_time          \n",
    "            \n",
    "            if eval_epoch_f1 > models_info[model.name]['best_f1']: #updates infos if current f1 is better then the previous one \n",
    "\n",
    "                models_info[model.name]['best_f1'] = eval_epoch_f1\n",
    "                models_info[model.name]['tot_pred'], models_info[model.name]['tot_targ'] =  tot_pred, tot_targ\n",
    "                models_info[model.name]['model'] = model \n",
    "                models_info[model.name]['epoch'] = epoch+1\n",
    "    \n",
    "\n",
    "            #log accuracy data on tensorboard  \n",
    "            writer.add_scalar(\"Train Acc\", train_epoch_acc, epoch+1)\n",
    "            writer.add_scalar(\"Val Acc\", eval_epoch_acc, epoch+1)\n",
    "            writer.add_scalar(\"Train F1\", train_epoch_f1, epoch+1)\n",
    "            writer.add_scalar(\"Val F1\", eval_epoch_f1, epoch+1)\n",
    "\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {tot_epoch_time:.4f}')\n",
    "            print(f'\\tTrain Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f}')\n",
    "            print(f'\\t Val. Loss: {eval_epoch_loss:.3f} | Eval. Acc: {eval_epoch_acc*100:.2f}% | Eval. F1: {eval_epoch_f1:.2f}')\n",
    "\n",
    "        writer.close()\n",
    "    \n",
    "    return models_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accoring to our experimental results obtained after an hyperparameter tuning phase, here are the ones that we are using to train each architecture:\n",
    "- `BATCH_SIZE = 16`: we have experimented with the sizes a little bit, but we don't want batches to be neither too small, to avoid noise in the gradients, nor too big, to speed up the process. Since we didn't see big differences for a batch size range between 8 and 32, we decided to set `BATCH_SIZE = 16`.\n",
    "- `LR = 0.4`: we obtained the best results using `Adam`, which automatically sets the initial learning rate. To instead use `SGD`, we found `LR = 0.4` to be quite good.\n",
    "- `HIDDEN_STATE = 128`: it is well known that setting an high dimension of the hidden layers would ensure to the networks the ability to represents better the informations between the states, with respect to a small hidden dimension. For this reason we tried three different dimensions: 64, 128 and 256. The results of the comparison are quite similar but confirm the assumption, and we see that an `HIDDEN_STATE = 128` well represents the data.\n",
    "- `N_EPOCHS = 15`: with a small enough batch size, `N_EPOCHS = 15` seems enough to obtain good results and avoid overfitting.\n",
    "- `USE_INIT_WEIGHTS = False`: as already described, we have inserted a weight initialization for the LSTM and FC layers. Unfortunately it seems that initializing the weights with `mean = 0` and `std = 0.1` doesn't actually help the convergence of the gradient. On the contrary, by setting `USE_INIT_WEIGHTS = False` allows to obtain an increase on the F1 score of 1-2% points.\n",
    "- `USE_CLASS_WEIGHTS = False`: if set to `True` the model would call the function `compute_class_weights` which we have defined in order use a weighted loss. Nevertheless, to train our model we decided to set it to `False`, the reasons will be explained in the error analysis section.\n",
    "- `OPTIMIZER = 'Adam'`: we have tried many different types of gradient descent optimizers, such as: `SGD`, `SGD` with `momentum`, `RMS-prop` and `Adam`. For this particular task, `Adam` seems to be the best choice, althought choosing the best configurations of hyperparameters for `SGD` with `momentum` could achieve comparable performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS, HYPERPARAMETERS AND USEFUL OBJECTS \n",
    "\n",
    "#param\n",
    "PAD_IDX = 0                     # pad index\n",
    "OUTPUT_DIM = len(tag2int)+1     # 0 is reserved \n",
    "\n",
    "#hyperparameters\n",
    "BATCH_SIZE = 16                 # number of sentences in each mini-batch\n",
    "LR = 0.4                        # learning rate (not used if OPTIMIZER = 'Adam') \n",
    "HIDDEN_STATE = 128              # number of nodes in hidden state\n",
    "N_EPOCHS = 15                   # number of epochs\n",
    "USE_INIT_WEIGHTS = False        # if `True` initialize weights and biases of LSTM, GRU and FC layers \n",
    "USE_CLASS_WEIGHTS = False       # if `True` balance weights of cross entropy loss \n",
    "OPTIMIZER = 'Adam'              # choose Gradient Descent optimizer to use ('Adam' or 'SGD')\n",
    "\n",
    "#parameters dictionary \n",
    "param = {'lr':LR,\n",
    "        'hidden_state':HIDDEN_STATE,\n",
    "        'n_epochs':N_EPOCHS,\n",
    "        'use_init_weights':USE_INIT_WEIGHTS,\n",
    "        'use_class_weights':USE_CLASS_WEIGHTS,\n",
    "        'optimizer':OPTIMIZER,\n",
    "        'pad_idx':PAD_IDX,\n",
    "        'output_dim':OUTPUT_DIM}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architectures that has to be evaluated must be placed in the `architectures` list of namedtuples. In particular, we tried:\n",
    "- `naive`: one embedding layer, one bidirectional LSTM layer and one FC layer.\n",
    "- `two_lstm`: one embedding layer, two bidirectional LSTM layers and one FC layer.\n",
    "- `two_dense`: one embedding layer, one LSTM layer and two FC layers.\n",
    "- `gru`: one embedding layer, one bidirectional GRU layer and one FC layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = namedtuple('architecture',['double_lstm','double_dense','use_gru'])\n",
    "\n",
    "naive = architecture(False,False,False)\n",
    "two_lstm = architecture(True,False,False)\n",
    "two_dense = architecture(False,True,False)\n",
    "gru = architecture(False,False,True)\n",
    "\n",
    "architectures = [naive,two_lstm,two_dense,gru]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally train our defined architectures and see how they perform over the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader,val_dataloader,test_dataloader = create_dataloaders(BATCH_SIZE)   #create dataloaders with given batch size \n",
    "models_info = train_and_eval(architectures,param,train_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMMENTA RISULTATI SOPRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained all our models, we can pick the two best ones according to the f1 score over the validation set and store them in the dictionary : `two_best_models`.\\\n",
    "We will later use this dictionary in order to further evaluate those models on the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the single best model is : use_GRU with 0.74 f1-score\n"
     ]
    }
   ],
   "source": [
    "ordered_models = dict(sorted(models_info.items(), key=lambda item: item[1]['best_f1'], reverse=True))    #order the models in models_info based on their f1_score \n",
    "\n",
    "two_best_models = OrderedDict()      #store in a dictionary the two best models \n",
    "\n",
    "for n,(k,v) in enumerate(ordered_models.items()):\n",
    "    two_best_models[k] = v\n",
    "    if n==1 : break            #take only the best two \n",
    "\n",
    "best_model_name = next(iter(two_best_models))    #we need the very best model to evaluate its errors \n",
    "best_model_info = two_best_models[best_model_name]\n",
    "\n",
    "best_f1, pred, targ = best_model_info['best_f1'],best_model_info['tot_pred'],best_model_info['tot_targ']\n",
    "\n",
    "print('the single best model is :',best_model_name,'with',round(best_f1,2),'f1-score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMMENTA F1 SOPRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Error analysis on Validation Set\n",
    "\n",
    "Now that we have trained all the models we can perform a simple error analysis about the results of the validation set, in order to understand the errors and think about possible improvements.\\\n",
    "In particular, we have developed the follow functions:\n",
    "- `build_classification_report`: create classification report showing the precision, recall, f1-score and support for each tag and the final accuracy.\n",
    "- `build_confusion_matrix`: create a confusion matrix.\n",
    "- `build_errors_dictionary`: create a dictionary which associates to each tag a list of the wrong tags that are predicted instead of the true one and the total number of times it happens for the validation set.\n",
    "- `get_tag_distribution`: show the number of occurrences of each tag in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def build_classification_report(targ,pred,unique_tags):\n",
    "    \"\"\"\n",
    "        Generate classification report \n",
    "    \"\"\"\n",
    "    report = classification_report(targ,pred,zero_division=0,output_dict=False,target_names=unique_tags)\n",
    "    print(report)\n",
    "\n",
    "\n",
    "def build_confusion_matrix(targ,pred,unique_tags):\n",
    "    \"\"\"\n",
    "        Build confusion matrix and returns it as a dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    cf_matrix = confusion_matrix(targ, pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix, index = unique_tags, columns = unique_tags)\n",
    "\n",
    "    #plot the confusion matrix as heatmap \n",
    "    plt.figure(figsize = (40,32))\n",
    "    sn.heatmap(df_cm, annot=True, cmap=\"Blues\", linewidths= 0.05, linecolor='white')\n",
    "\n",
    "    return df_cm\n",
    "\n",
    "def build_errors_dictionary(df_cm):\n",
    "    \"\"\"\n",
    "        Create a dictionary which contains, for each true tag a list of wrong predicted tags and the number of wrong predictions\n",
    "    \"\"\"\n",
    "\n",
    "    errors = {}\n",
    "    for true_tag,row in df_cm.iterrows():\n",
    "\n",
    "        tag_errors = []\n",
    "        for pred_tag, occurrences in row.iteritems() : \n",
    "            if not pred_tag==true_tag and occurrences!=0 : \n",
    "                tag_errors.append((pred_tag,occurrences))\n",
    "        \n",
    "        tag_errors.sort(key = itemgetter(1),reverse=True)\n",
    "\n",
    "        if tag_errors:     \n",
    "            errors[true_tag] = tag_errors\n",
    "\n",
    "    errors = dict(sorted(errors.items(), key = lambda item : item[1][0][1],reverse=True))    #sort the dictionary in order to have the more wrongly classified tags on top \n",
    "\n",
    "    #pretty print \n",
    "    print('true_TAG --> (pred_TAG, n_times)\\n')\n",
    "    for k,v in errors.items():\n",
    "        print(k,'-->',*v)\n",
    "\n",
    "def get_tag_distribution():\n",
    "    \"\"\"\n",
    "        Count occurrences of each TAG in the train set \n",
    "    \"\"\"\n",
    "    tag_frequency = {}\n",
    "    df_temp = token_dataset[token_dataset['split'] == 'train']\n",
    "    for _ ,row in df_temp.iterrows():\n",
    "        for key in row['tags_token']:\n",
    "            tag_frequency[int2tag[key]] = tag_frequency.get(int2tag[key],0) + 1\n",
    "\n",
    "    return dict(sorted(tag_frequency.items(), key=lambda item: item[1], reverse = True))\n",
    "\n",
    "def plot_tag_occur(occ: Dict):\n",
    "    \"\"\"\n",
    "        Creates and plots an image showing the distribution of each unique tag inside the dataset\n",
    "    \"\"\"\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_ylabel('num occurrences in train dataset')\n",
    "    ax.set_xticks(range(len(occ.keys())))\n",
    "    ax.bar(occ.keys(), occ.values())\n",
    "\n",
    "    plt.title(\"Tags distribution in train dataset\")\n",
    "    fig.set_figwidth(25)\n",
    "    fig.set_figheight(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell computes the number of unique tags which have been predicted well or wrongly (removing as usual the punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [int2tag[i] for i in set(targ.tolist() + pred.tolist())]                           #tags as string instead of indexes, for every unique tag in pred or tag \n",
    "print('the number of unique tags in either the predictions or targets is: ',len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training the model for few epochs, it can be seen how the number of unique tags (true + predicted) are greater than 37 (number of total tags without punctuation). This behaviour is justified by the fact that the model performs really bad and predict wrongly as punctuation some words which aren't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_classification_report(targ,pred,tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report is great to understand what are the main tags that we predict wrongly most of the times.\\\n",
    "By looking at the plot, we can see two bad behaviours:\n",
    "- Some tags have high precision but low recall, so we predict them well but we also produce a lot of false positives.\n",
    "- Other tags instead have low precision and low recall so we are completely wrong in predicting them.\n",
    "\n",
    "By making a comparison between these two types of tags, it can be seen how both have a low support, so there are not many of them in the train dataframe.\n",
    "\\\n",
    "\\\n",
    "For this reason, we have developed the function `compute_class_weights` which add weights to the cross entropy loss. It implements the homonymous one of `sklearn` which give more weight to the prediction errors of class underrepresented in the train dataframe, and less to the ones overrepressented. \n",
    "\\\n",
    "\\\n",
    "Although the function does what it is supposed to do, effectively reducing prediction errors for the less represented classes, the result is now however also an increase in prediction errors among the more represented classes, unfortunately offsetting the positive effects. Therefore, we decided not to use the function during the training of the models.\n",
    "\\\n",
    "\\\n",
    "A possible improvement that would certainly lead to better results would be to increase the weight of prediction errors on underrepresented classes by balancing it with that of more represented classes, so as not to create substantial differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = build_confusion_matrix(targ, pred, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is useful to understand which tags we get wrong the most and with which we predict them wrongly.\\\n",
    "In order to have a more understandable result of the errors we can use the `build_errors_dictionary` function that we have previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_errors_dictionary(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set a threshold of nearly 70 recurrent fail predictions, and by looking at the results above these are the ones most relevant:\n",
    "- `NN-JJ`: prediction of nouns as adjectives (because of a lot of examples of `NN` but few of `JJ`) and viceversa.\n",
    "- `NN-NNP`: prediction of singular nouns as singular proper nouns (because very similar).\n",
    "- `JJ-NNP`: prediction of adjectives as singular proper nouns (because of a lot of examples of `NNP` but few of `JJ`) and viceversa.\n",
    "- `NNPS-NNP`: prediction of plural proper nouns as singular proper nouns (because of few examples of `NNPS` but a lot of `NNP`).\n",
    "- `NNS-NN`: prediction of plural nouns as singular nouns (because of a lot of examples of `NN` but few of `NNS`) and viceversa.\n",
    "\n",
    "On the other hand, it's important to take a look also to fail predictions which occur rarely, because the cause is often that they are present only in few sequences, such as:\n",
    "- `LS-CD`: few times wrong prediction of cardinal numbers as list item marker (because of few examples of `LS` but a lot of `CD`).\n",
    "- `FW`: fail in predicting foreign words (because of few examples).\n",
    "- `UH`: fail in predicting interjections (because of few examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand which are the most frequent tags within the dataset and which are less frequent, we can plot the distribution of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dis = get_tag_distribution()\n",
    "\n",
    "plot_tag_occur(tag_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot reconfirms what has been previously stated, so that there are classes highly represented and others with almost zero examples. The main errors will be made between similar classes but with a big difference of representation in the dataset and on classes not very represented in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Test of two best models\n",
    "\n",
    "After evaluating the mistakes made on the validation set and improving the models accordingly, it is now time to evaluate the two best models using the sentences of the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX, weight = compute_class_weights() if USE_CLASS_WEIGHTS else None)\n",
    "\n",
    "test_pred, test_targ = None, None        #store the predicted labels and the true labels for the best model on test set \n",
    " \n",
    "for model_name,model_info in two_best_models.items():\n",
    "    model = model_info['model']\n",
    "    loss, acc, f1, tot_pred, tot_targ = eval_loop(model,test_dataloader,criterion,PAD_IDX)\n",
    "\n",
    "    print(model_name,f': Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}% | Test F1: {f1:.2f}')\n",
    "\n",
    "    if model_name == best_model_name:\n",
    "        test_pred, test_targ = tot_pred, tot_targ\n",
    "\n",
    "tags = [int2tag[i] for i in set(test_targ.tolist() + test_pred.tolist())]                           #tags as string instead of indexes, for every unique tag in pred or tag \n",
    "print('\\nthe number of unique tags in either the predictions or targets is: ',len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_classification_report(test_targ,test_pred,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = build_confusion_matrix(test_targ, test_pred, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_errors_dictionary(df_cm)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b10856870a6af5fec4ffddd4d7318a6f2add2c9f3b4bd7caecf75cea33b7bd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
