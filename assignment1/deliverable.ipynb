{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to print all output for a cell instead of only last one \n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import and fixed seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import string\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "import time \n",
    "\n",
    "# typing\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current work directory: {}\".format(os.getcwd()))\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download data (dataset and glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source of this code -> https://gist.github.com/hantoine/c4fc70b32c2d163f604a8dc2a050d5f6 \n",
    "\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def download_and_unzip_dataset():\n",
    "\n",
    "    dataset_folder = os.path.join(data_folder,\"dependency_treebank\")\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "    if not os.path.exists(dataset_folder):\n",
    "        print('downloading and extracting dataset to :',dataset_folder)\n",
    "        with urlopen(url) as response:\n",
    "            zipfile = ZipFile(BytesIO(response.read()))\n",
    "            zipfile.extractall(path=data_folder)\n",
    "    else :\n",
    "        print(\"the dataset has been already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode dataset in pandas dataframe \n",
    "\n",
    "def encode_dataset(dataset_name: str) -> pd.DataFrame:\n",
    "\n",
    "    print(\"Encoding dataset as pandas dataframe...\")\n",
    "\n",
    "    dataset_folder = os.path.join(data_folder,\"dependency_treebank\")\n",
    "    \n",
    "    dataframe_rows = []             #dataframe that will contain all the sentences in all the documents, each sentence as a list of word and a list of corresponding tags\n",
    "    unique_tags = set()\n",
    "    unique_words = set()\n",
    "\n",
    "    for doc in os.listdir(dataset_folder):\n",
    "      doc_num = int(doc[5:8])\n",
    "      doc_path = os.path.join(dataset_folder,doc)\n",
    "\n",
    "      with open(doc_path, mode='r', encoding='utf-8') as file:\n",
    "        df = pd.read_csv(file,sep='\\t',header=None,skip_blank_lines=False)\n",
    "        df.rename(columns={0:'word',1:\"TAG\",2:\"remove\"},inplace=True)\n",
    "        df.drop(\"remove\",axis=1,inplace=True)\n",
    "        \n",
    "        #create another column that indicate group by sentence \n",
    "        df[\"group_num\"] = df.isnull().all(axis=1).cumsum()\n",
    "        df.dropna(inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        unique_tags.update(df['TAG'].unique())     #save all the unique tags in a set \n",
    "        unique_words.update(df['word'].unique())   #save all the unique words in a set \n",
    "\n",
    "        #generate sentence list in a document \n",
    "        df_list = [df.iloc[rows] for _, rows in df.groupby('group_num').groups.items()]\n",
    "        for n,d in enumerate(df_list) :           #for each sentence create a row in the final dataframe\n",
    "            dataframe_row = {\n",
    "                \"split\" : 'train' if doc_num<=100 else ('val' if doc_num<=150  else 'test'),\n",
    "                \"doc_id\" : doc_num,\n",
    "                \"sentence_num\" : n,\n",
    "                \"words\": d['word'].tolist(),\n",
    "                \"tags\":  d['TAG'].tolist(),\n",
    "                \"num_tokens\": len(d['word'])\n",
    "            }\n",
    "            dataframe_rows.append(dataframe_row)\n",
    "\n",
    "    dataframe_path = os.path.join(data_folder, dataset_name)\n",
    "    df_final = pd.DataFrame(dataframe_rows)\n",
    "    df_final.to_csv(dataframe_path + \".csv\")                      #save as csv to inspect\n",
    "\n",
    "    print(\"Encoding completed!\")\n",
    "      \n",
    "    return  df_final, unique_tags, unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "#build the dictionaries that will be used for the embedding matrix and one hot encoding of TAGS\n",
    "#starting from 1, 0 is reserved to PAD\n",
    "\n",
    "def build_dict(words : list[str], tags : list[str]): \n",
    "    \n",
    "    word2int = OrderedDict()\n",
    "    int2word = OrderedDict()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        word2int[word] = i+1\n",
    "        int2word[i+1] = word\n",
    "\n",
    "    tag2int = OrderedDict()\n",
    "    int2tag = OrderedDict()\n",
    "\n",
    "    for i, tag in enumerate(tags):\n",
    "        tag2int[tag] = i+1\n",
    "        int2tag[i+1] = tag\n",
    "    \n",
    "    print('saving dictionaries as pickle files')\n",
    "    pickle_files = [word2int,int2word,tag2int,int2tag]\n",
    "    files_path = os.path.join(data_folder,'dictionaries.pkl')\n",
    "    with open(files_path, 'wb') as f:\n",
    "        pickle.dump(pickle_files, f)\n",
    "\n",
    "    return word2int,int2word,tag2int,int2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: cosa fa ? \n",
    "def build_tokenized_dataframe(word2int: Dict,tag2int: Dict, df : pd.DataFrame):\n",
    "\n",
    "    print('Initiating tokenization of words and tags in dataframe')\n",
    "    tokenized_rows = []\n",
    "    for words,tags in zip(df['words'],df['tags']):\n",
    "        tokenized_row = {'words_token':[word2int[word] for word in words ],'tags_token':[tag2int[tag] for tag in tags ]}\n",
    "        tokenized_rows.append(tokenized_row)\n",
    "    \n",
    "    tokenized_df = pd.DataFrame(tokenized_rows)\n",
    "\n",
    "    tokenized_df.insert(0,'split',df['split'])\n",
    "    tokenized_df.insert(1,'num_tokens',df['num_tokens'])\n",
    "\n",
    "    print('Tokenization completed')\n",
    "\n",
    "    return tokenized_df\n",
    "\n",
    "#TODO: cosa fa ?\n",
    "def check_dataframe_tokenization(tokenized_df, normal_df, int2word, int2tag) :\n",
    "\n",
    "    for n, (w_t, t_t) in enumerate(zip(tokenized_df['words_token'],tokenized_df['tags_token'])):\n",
    "        if not normal_df.loc[n,'words'] == [int2word[word_token] for word_token in w_t]:\n",
    "            print('words tokenization gone wrong') \n",
    "            return False\n",
    "        if not normal_df.loc[n,'tags'] == [int2tag[tag_token] for tag_token in t_t]:\n",
    "            print('tags tokenization gone wrong')\n",
    "            return False \n",
    "    \n",
    "    print('all right with dataset tokenization')\n",
    "    print('saving tokenized dataframe')\n",
    "    path = os.path.join(data_folder, \"token_dataset\")\n",
    "    tokenized_df.to_pickle(path+'.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_glove_emb():   \n",
    "    \n",
    "    print('downloading glove embeddings ')        \n",
    "    embedding_dimension=300\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    emb_model = gloader.load(download_path)\n",
    "    \n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, unique_words: list[str], lower: bool):\n",
    "\n",
    "    oov_words = []\n",
    "\n",
    "    if lower:\n",
    "        words = set([x.lower() for x in unique_words])\n",
    "    else: \n",
    "        words = unique_words\n",
    "\n",
    "    for word in words:\n",
    "        try: \n",
    "           embedding_model[word]\n",
    "        except:\n",
    "           oov_words.append(word) \n",
    "    \n",
    "    print(\"Total number of unique words in dataset:\",len(words))\n",
    "    print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(words))*100))\n",
    "    print(\"Some OOV terms:\",random.sample(oov_words,15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value_distribution_glove(glove: gensim.models.keyedvectors.KeyedVectors):\n",
    "    max_v = np.max([(np.max(glove[i])) for i in range(len(glove))])\n",
    "    min_v = np.min([(np.min(glove[i])) for i in range(len(glove))])\n",
    "\n",
    "    print('Max value inside glove embeddings:',max_v)\n",
    "    print('Min value inside glove embeddings:',min_v)\n",
    "\n",
    "#TODO cosa fa?\n",
    "\n",
    "def build_embedding_matrix(emb_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                           word2int: Dict[str, int]) -> np.ndarray:\n",
    "    \n",
    "    check_value_distribution_glove(emb_model)\n",
    "   \n",
    "    embedding_dimension = len(emb_model[0])                                                              \n",
    "    embedding_matrix = np.zeros((len(word2int)+1, embedding_dimension), dtype=np.float32)\n",
    "\n",
    "    for word, idx in word2int.items():\n",
    "        try:\n",
    "            embedding_vector = emb_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    \n",
    "    print('Saving emb matrix to pickle file')\n",
    "    path = os.path.join(data_folder, \"emb_matrix\")\n",
    "    np.save(path,embedding_matrix,allow_pickle=True)\n",
    "\n",
    "    print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that the tokenized dataframe and the index of embeddings matrix correspond \n",
    "\n",
    "def check_id_corr(int2word : Dict[int,str],glove: gensim.models.keyedvectors.KeyedVectors, matrix, dataframe ):\n",
    "    \n",
    "    oov_words_ = []\n",
    "\n",
    "    for token_sentence in dataframe['words_token']:\n",
    "\n",
    "        for token in token_sentence:\n",
    "            emb1 = matrix[token]\n",
    "            word = int2word[token]\n",
    "            emb2 = None\n",
    "            try:\n",
    "                emb2 = glove[word]\n",
    "            except:\n",
    "                oov_words_.append(word)\n",
    "            if emb2 is not None:\n",
    "                assert(np.array_equal(emb1,emb2))\n",
    "\n",
    "    print('Double check OOV number:',len(set(oov_words_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_folder):\n",
    "    print('This is the first run! Data still not present')\n",
    "\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "    download_and_unzip_dataset()\n",
    "\n",
    "    df, unique_tags, unique_words = encode_dataset(\"dataset\")\n",
    "\n",
    "    word2int,int2word,tag2int,int2tag = build_dict(unique_words,unique_tags)\n",
    "\n",
    "    tokenized_df = build_tokenized_dataframe(word2int,tag2int,df)\n",
    "\n",
    "    check_dataframe_tokenization(tokenized_df,df, int2word, int2tag)\n",
    "\n",
    "    glove_embeddings = download_glove_emb()\n",
    "\n",
    "    check_OOV_terms(glove_embeddings, unique_words,False)\n",
    "\n",
    "    embedding_matrix = build_embedding_matrix(glove_embeddings, word2int)\n",
    "    \n",
    "    check_id_corr(int2word,glove_embeddings,embedding_matrix,tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    emb_matrix_path = os.path.join(data_folder,'emb_matrix.npy')\n",
    "    token_dataset_path = os.path.join(data_folder,'token_dataset.pkl')\n",
    "    dictionaries_path = os.path.join(data_folder,'dictionaries.pkl')\n",
    "\n",
    "    if os.path.exists(emb_matrix_path) and os.path.exists(token_dataset_path):\n",
    "        print('loading embedding matrix')\n",
    "        emb_matrix = np.load(emb_matrix_path,allow_pickle=True)\n",
    "        print('loading tokenized dataset')\n",
    "        token_dataset = pd.read_pickle(token_dataset_path)\n",
    "        print('loading dictionaries')\n",
    "        with open(dictionaries_path, 'rb') as f:\n",
    "            word2int,int2word,tag2int,int2tag = pickle.load(f)\n",
    "        \n",
    "        print('all data loaded')\n",
    "    else:\n",
    "        print('searched data is not present in folder')\n",
    "        emb_matrix, token_dataset = None, None\n",
    "\n",
    "    return emb_matrix, token_dataset, word2int, int2word, tag2int, int2tag\n",
    "\n",
    "emb_matrix, token_dataset, word2int,int2word,tag2int,int2tag = load_data()\n",
    "\n",
    "token_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytoch import \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.legacy.data import BucketIterator\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix: np.ndarray, pad_idx : int):\n",
    "    matrix = torch.Tensor(weights_matrix)\n",
    "    _ , embedding_dim = matrix.shape \n",
    "    emb_layer = nn.Embedding.from_pretrained(matrix, freeze=True, padding_idx = pad_idx)\n",
    "    \n",
    "    return emb_layer, embedding_dim\n",
    "\n",
    "class custom_model(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_matrix : np.ndarray, hidden_dim: int, tag_output_dim: int, pad_idx: int, num_lstm : int, double_dense : bool, use_GRU: bool) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding, embedding_dim = create_emb_layer(emb_matrix,pad_idx)\n",
    "\n",
    "        if use_GRU:\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first = True, num_layers = num_lstm, bidirectional = True)\n",
    "        else :\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first = True, num_layers = num_lstm, bidirectional = True)\n",
    "\n",
    "        self.middle_dense = None \n",
    "\n",
    "        if double_dense:\n",
    "            self.middle_dense = nn.Linear(hidden_dim*2,hidden_dim)\n",
    "            self.hidden2tag = nn.Linear(hidden_dim, tag_output_dim)\n",
    "        else :\n",
    "            self.hidden2tag = nn.Linear(hidden_dim * 2 , tag_output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeds = self.embedding(sentences)\n",
    "        out, _  = self.rnn(embeds)\n",
    "        if self.middle_dense is not None :\n",
    "            out = self.middle_dense(out)\n",
    "        tag_space = self.hidden2tag(out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataframeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "        self.X = dataframe['words_token']\n",
    "        self.y = dataframe['tags_token']\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return {'words': self.X[idx],'tags': self.y[idx]}\n",
    "\n",
    "def create_dataloaders(b_s : int):     #b_s = batch_size\n",
    "\n",
    "    train_df = token_dataset[token_dataset['split'] == 'train'].reset_index()\n",
    "    val_df = token_dataset[token_dataset['split'] == 'val'].reset_index()\n",
    "    test_df = token_dataset[token_dataset['split'] == 'test'].reset_index()\n",
    "\n",
    "    train_dataset = DataframeDataset(train_df)\n",
    "    val_dataset = DataframeDataset(val_df)\n",
    "    test_dataset = DataframeDataset(test_df)\n",
    "\n",
    "\n",
    "    # Group similar length text sequences together in batches.\n",
    "    train_dataloader,val_dataloader,test_dataloader = BucketIterator.splits((train_dataset,val_dataset,test_dataset),\n",
    "                                                        batch_sizes=(b_s,b_s,b_s), sort_key=lambda x: len(x['words']), \n",
    "                                                        repeat=True, sort=False, shuffle=True, sort_within_batch=True)\n",
    "    \n",
    "    return train_dataloader,val_dataloader,test_dataloader \n",
    "\n",
    "\n",
    "def check_data_loaders(train_dataloader,val_dataloader,test_dataloader):\n",
    "\n",
    "    for n,dataloader in enumerate((train_dataloader,val_dataloader,test_dataloader)):\n",
    "\n",
    "        dataloader.create_batches() # Create batches - needs to be called before each loop.\n",
    "\n",
    "        max_diff = -1\n",
    "        for batch in dataloader.batches:\n",
    "\n",
    "            min = np.min([len(example['words']) for example in batch])\n",
    "            max = np.max([len(example['words']) for example in batch])\n",
    "\n",
    "            diff = max - min\n",
    "\n",
    "            if diff > max_diff: max_diff = diff \n",
    "        \n",
    "        s = 'train' if n==0 else ('val' if n==1 else 'test')\n",
    "        \n",
    "        print('in',s+'_dataloader the maximum difference in number of tokens between two sentences in the same batch is:',max_diff)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    #print random sentence from train_dataloader\n",
    "    from operator import itemgetter\n",
    "    train_dataloader.create_batches()\n",
    "    for batch in train_dataloader.batches:\n",
    "        for example in batch:\n",
    "            print(*example['words'])\n",
    "            print('random sentence from train_dataloader:')\n",
    "            print(itemgetter(*example['words'])(int2word))\n",
    "            print(itemgetter(*example['tags'])(int2tag))\n",
    "            break\n",
    "        break\n",
    "\n",
    "#tr_d, va_d, te_d = create_dataloaders(64)     #TODO only for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_data_loaders(tr_d,va_d,te_d)     #TODO only for testing (keep in a different cell to avoid recreating everytime dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return a torch tensor which contains the indexes of the tags that we don't want to evaluate (punctuation)\n",
    "def get_to_be_masked_tags():\n",
    "\n",
    "    punctuation_tags = ['$', '``', '.', ',', '#', 'SYM', ':', \"''\"]\n",
    "    token_punctuations = [tag2int[tag] for tag in punctuation_tags]    \n",
    "\n",
    "    print('the indexes of punct tags:',token_punctuations) # int of punctuation's tokens\n",
    "    print([int2tag[token_int] for token_int in token_punctuations]) # TODO only for testing\n",
    "\n",
    "    return torch.LongTensor(token_punctuations+[0]) #0 is the pad token \n",
    "\n",
    "to_mask = get_to_be_masked_tags()\n",
    "\n",
    "#return two tensors : the predicted labels and the true labels, both removing unwanted classes \n",
    "def reshape_and_mask(predictions: torch.Tensor,targets: torch.LongTensor):\n",
    "\n",
    "    max_preds = predictions.argmax(dim=1)\n",
    "    non_masked_elements = torch.isin(targets, to_mask, invert=True)\n",
    "    \n",
    "    return max_preds[non_masked_elements],targets[non_masked_elements]\n",
    "\n",
    "def acc_and_f1(y_pred: torch.LongTensor, y_true: torch.LongTensor):\n",
    "\n",
    "    correct = y_pred.eq(y_true)\n",
    "    acc = correct.sum()/y_true.shape[0] \n",
    "\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    return acc,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: nn.Module, iterator : BucketIterator, optimizer: optim.Optimizer, criterion, pad_idx : int):\n",
    "\n",
    "    batch_loss = 0\n",
    "    \n",
    "    tot_pred , tot_targ = torch.LongTensor(), torch.LongTensor()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    iterator.create_batches()\n",
    "\n",
    "    for batch_id, batch in enumerate(iterator.batches):\n",
    "\n",
    "        batch_X = [torch.LongTensor(example['words']) for example in batch]\n",
    "        batch_y = [torch.LongTensor(example['tags']) for example in batch]\n",
    "\n",
    "        padded_X = rnn.pad_sequence(batch_X, batch_first = True, padding_value = pad_idx)\n",
    "        padded_y = rnn.pad_sequence(batch_y, batch_first = True, padding_value = pad_idx)\n",
    "\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        optimizer.zero_grad() #TODO forse non serve ne basta uno dei due \n",
    "\n",
    "        predictions = model(padded_X)\n",
    "        predictions = predictions.view(-1,predictions.shape[-1])\n",
    "        targets = padded_y.view(-1)\n",
    "\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        pred, targ = reshape_and_mask(predictions,targets)\n",
    "        tot_pred = torch.cat((tot_pred,pred))\n",
    "        tot_targ = torch.cat((tot_targ,targ))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "\n",
    "    epoch_loss = batch_loss/(batch_id+1)\n",
    "    epoch_acc, epoch_f1 = acc_and_f1(tot_pred,tot_targ)\n",
    "\n",
    "    return epoch_loss,epoch_acc,epoch_f1\n",
    "\n",
    "\n",
    "def eval_loop(model: nn.Module, iterator: BucketIterator, criterion, pad_idx):\n",
    "    \n",
    "    batch_loss = 0\n",
    "    \n",
    "    tot_pred , tot_targ = torch.LongTensor(), torch.LongTensor()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    iterator.create_batches()\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_id, batch in enumerate(iterator.batches):\n",
    "\n",
    "            batch_X = [torch.LongTensor(example['words']) for example in batch]\n",
    "            batch_y = [torch.LongTensor(example['tags']) for example in batch]\n",
    "\n",
    "            padded_X = rnn.pad_sequence(batch_X, batch_first = True, padding_value = pad_idx)\n",
    "            padded_y = rnn.pad_sequence(batch_y, batch_first = True, padding_value = pad_idx)\n",
    "            \n",
    "            predictions = model(padded_X)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            targets = padded_y.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, targets)\n",
    "            \n",
    "            pred, targ = reshape_and_mask(predictions,targets)\n",
    "            tot_pred = torch.cat((tot_pred,pred))\n",
    "            tot_targ = torch.cat((tot_targ,targ))\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "\n",
    "    epoch_loss = batch_loss/(batch_id+1)\n",
    "    epoch_acc, epoch_f1 = acc_and_f1(tot_pred,tot_targ)\n",
    "\n",
    "    return epoch_loss,epoch_acc,epoch_f1,tot_pred,tot_targ\n",
    "\n",
    "\n",
    "def train_and_eval(n_epochs,model,optimizer,criterion,t_d,v_d,pad_idx):\n",
    "    \n",
    "    best_val_f1 = -1\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_epoch_loss, train_epoch_acc, train_epoch_f1 = train_loop(model, t_d, optimizer, criterion, pad_idx)\n",
    "        val_epoch_loss, val_epoch_acc, val_epoch_f1, tot_pred, tot_targ = eval_loop(model, v_d, criterion, pad_idx)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        tot_epoch_time = end_time-start_time           #TODO : STAMPARE IL TEMPO \n",
    "\n",
    "        \n",
    "        if val_epoch_f1 > best_val_f1:\n",
    "            best_val_f1 = val_epoch_f1\n",
    "            best_pred, best_targ = tot_pred, tot_targ\n",
    "            if not os.path.exists('models'):\n",
    "                os.makedirs('models')\n",
    "            torch.save(model.state_dict(), 'models/model.pt')\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {tot_epoch_time:.4f}')\n",
    "        print(f'\\tTrain Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f}')\n",
    "        print(f'\\t Val. Loss: {val_epoch_loss:.3f} | Val. Acc: {val_epoch_acc*100:.2f}% | Val. F1: {val_epoch_f1:.2f}')\n",
    "    \n",
    "    return best_pred, best_targ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS AND OBJECTS \n",
    "\n",
    "device = torch.device('cuda' if False else 'cpu')   #torch.cuda.is_available()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "PAD_IDX = 0\n",
    "LSTM_LAYER = 1\n",
    "DOUBLE_DENSE = False\n",
    "USE_GRU = False\n",
    "LR = 0.5\n",
    "HIDDEN_STATE = 128\n",
    "OUTPUT_DIM = len(tag2int)+1\n",
    "N_EPOCHS = 30\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "model = custom_model(emb_matrix,HIDDEN_STATE,OUTPUT_DIM,PAD_IDX,LSTM_LAYER,DOUBLE_DENSE,USE_GRU)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr = LR)\n",
    "\n",
    "train_dataloader,val_dataloader,test_dataloader = create_dataloaders(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, targ = train_and_eval(N_EPOCHS,model,optimizer,criterion,train_dataloader,val_dataloader,PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST PER PROVARE ROBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTS\n",
    "w_matrix = np.array([[0,0,0,0],[1,5,5,7],[2,4,7,8],[3,5,7,6],[6,3,5,4],[3,4,6,3]])\n",
    "m = torch.Tensor(w_matrix)\n",
    "l = nn.Embedding.from_pretrained(m, freeze=True, padding_idx = 0)\n",
    "\n",
    "\n",
    "a = torch.LongTensor([[2,1,0],[3,2,3],[2,5,0]])    #input\n",
    "c = torch.LongTensor([[2,3,0],[3,2,3],[1,4,0]])    #target\n",
    "print('a shape',a.shape)\n",
    "print('c shape',c.shape)\n",
    "\n",
    "\n",
    "a1 = l(a)\n",
    "print('a1 shape',a1.shape)\n",
    "\n",
    "\n",
    "a2 = a1.view(-1,a1.shape[-1])\n",
    "c1 = c.view(-1)\n",
    "print('a2 shape',a2.shape)\n",
    "print('a1 shape',c1.shape)\n",
    "\n",
    "\n",
    "a3= a2.argmax(dim=1)\n",
    "print('a3 shape',a3.shape)\n",
    "\n",
    "print('a1',a1)\n",
    "print('a2',a2)\n",
    "print('a3',a3)\n",
    "print('c1',c1)\n",
    "\n",
    "\n",
    "\n",
    "non_mask_elements = torch.isin(c1,torch.Tensor([0,1]),invert=True)\n",
    "non_mask_elements\n",
    "a4= a3[non_mask_elements]\n",
    "c2= c1[non_mask_elements]\n",
    "print('a4',a4)\n",
    "print('c2',c2)\n",
    "\n",
    "correct = a4.eq(c2)\n",
    "b = correct.sum()/c2.shape[0]\n",
    "b\n",
    "b+0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_pred , tot_targ = torch.LongTensor(),torch.LongTensor()\n",
    "tot_pred = torch.cat((tot_pred,c2))\n",
    "torch.cat((tot_pred,a4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #return accuracy leaving out unwanted classes (pad_idx and punctuation)\n",
    "# def acc_masked(predictions,targets):\n",
    "\n",
    "#     max_preds = predictions.argmax(dim=1)\n",
    "#     non_masked_elements = torch.isin(targets,to_mask,invert=True)\n",
    "#     correct = max_preds[non_masked_elements].eq(targets[non_masked_elements])\n",
    "    \n",
    "#     return correct.sum()/targets[non_masked_elements].shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(int2tag.keys())\n",
    "max(int2tag.keys())\n",
    "len(int2tag)\n",
    "\n",
    "\n",
    "# l = functional.log_softmax(a1,dim=1)\n",
    "# l.shape\n",
    "# l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.create_batches()\n",
    "\n",
    "for batch_id, batch in enumerate(train_dataloader.batches):\n",
    "    \n",
    "    \n",
    "    min_len = np.min([len(torch.Tensor(example['words'])) for example in batch])\n",
    "    max_len = np.max([len(torch.Tensor(example['words'])) for example in batch])\n",
    "    \n",
    "    batch_X = [torch.LongTensor(example['words']) for example in batch]\n",
    "    batch_y = [torch.LongTensor(example['tags']) for example in batch]\n",
    "\n",
    "    padded_X = rnn.pad_sequence(batch_X, batch_first = True, padding_value = PAD_IDX)\n",
    "    padded_y = rnn.pad_sequence(batch_y, batch_first = True, padding_value = PAD_IDX)\n",
    "\n",
    "    print(padded_X.shape)\n",
    "    print(padded_y.shape)\n",
    "    # print(padded_X.is_cuda)\n",
    "    \n",
    "    # print(type(batch_X))\n",
    "    # print(type(padded_X))\n",
    "    # print(min_len, max_len)\n",
    "    # print(padded_X.shape)\n",
    "    # print(padded_y.shape)\n",
    "\n",
    "    # prediction = model(batch)\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b10856870a6af5fec4ffddd4d7318a6f2add2c9f3b4bd7caecf75cea33b7bd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
