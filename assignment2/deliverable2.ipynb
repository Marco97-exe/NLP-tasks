{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fact checking, Neural Languange Inference (NLI)**\n",
    "\n",
    "**Authors**: Giacomo Berselli, Marco Cuc√®, Riccardo De Matteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries and Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "import time \n",
    "\n",
    "# Fix data seed to achieve reproducible results\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current work directory: {}\".format(os.getcwd())) #print the current working directory \n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\") # directory containing the notebook\n",
    "\n",
    "if not os.path.exists(data_folder):   #create folder where all data will be stored \n",
    "    os.makedirs(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing first, we download the raw dataset, unzip it and store the csv document of each split in the dataset folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_path = os.path.join(data_folder,'raw_dataset')   #path of the raw dataset as downloaded \n",
    "\n",
    "def save_response_content(response, destination):    \n",
    "    CHUNK_SIZE =32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks                \n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(data_folder):\n",
    "    zip_dataset_path = os.path.join(raw_dataset_path,'fever_data.zip')    \n",
    "    data_url_id =\"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"    \n",
    "    url =\"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(raw_dataset_path):        \n",
    "        os.makedirs(raw_dataset_path)\n",
    "\n",
    "    if not os.path.exists(zip_dataset_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:           \n",
    "            response = current_session.get(url, params={'id': data_url_id}, stream=True)\n",
    "\n",
    "        save_response_content(response, zip_dataset_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_dataset_path) as loaded_zip:            \n",
    "            loaded_zip.extractall(raw_dataset_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "download_data(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the csv files of the train, val and test splits, we encode all three as a unique pandas Dataframe to be able to better inspect it and manipulate it as a whole.\n",
    "The Dataframe `df` is structured as follows: \n",
    "- `claim`: the fact to verify \n",
    "- `evidence`: one of the possibly multiple sentences in the dataset which supports or refutes the `claim`\n",
    "- `id`: number associated to the fact to verify (different rows can have the same `id`)\n",
    "- `label`: wether the evidence REFUTES or SUPPORTS the claim\n",
    "- `split`: the split to which one claim belongs (train, val, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the entire dataset in a pandas dataframe and add the split column\n",
    "def encode_dataset(): \n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for split in ['train','val','test']:\n",
    "        split_path = os.path.join(raw_dataset_path,f\"{split}_pairs.csv\")\n",
    "        split_df = pd.read_csv(split_path,index_col=0)\n",
    "        split_df['split'] = split\n",
    "\n",
    "        df = df.append(split_df,ignore_index=True,)\n",
    "\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    return df \n",
    "\n",
    "df = encode_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataframe shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['split']=='train'])\n",
    "len(df[df['split']=='val'])\n",
    "len(df[df['split']=='test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'].unique()\n",
    "df['label'].unique()\n",
    "df['id'].nunique()\n",
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b10856870a6af5fec4ffddd4d7318a6f2add2c9f3b4bd7caecf75cea33b7bd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
