{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fact checking, Neural Languange Inference (NLI)**\n",
    "\n",
    "**Authors**: Giacomo Berselli, Marco Cucè, Riccardo De Matteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries and Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c643c26490>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import random\n",
    "import string \n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "import time \n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "# Fix data seed to achieve reproducible results\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: c:\\Users\\Riccardo\\Project\\GitRepo\\NLP-assignments\\assignment2\n"
     ]
    }
   ],
   "source": [
    "print(\"Current work directory: {}\".format(os.getcwd())) #print the current working directory \n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\") # directory containing the notebook\n",
    "\n",
    "if not os.path.exists(data_folder):   #create folder where all data will be stored \n",
    "    os.makedirs(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing first, we download the raw dataset, unzip it and store the csv document of each split in the dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_path = os.path.join(data_folder,'raw_dataset')   #path of the raw dataset as downloaded \n",
    "\n",
    "def save_response_content(response, destination):    \n",
    "    CHUNK_SIZE =32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks                \n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(data_folder):\n",
    "    zip_dataset_path = os.path.join(raw_dataset_path,'fever_data.zip')    \n",
    "    data_url_id =\"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"    \n",
    "    url =\"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(raw_dataset_path):        \n",
    "        os.makedirs(raw_dataset_path)\n",
    "\n",
    "    if not os.path.exists(zip_dataset_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:           \n",
    "            response = current_session.get(url, params={'id': data_url_id}, stream=True)\n",
    "\n",
    "        save_response_content(response, zip_dataset_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_dataset_path) as loaded_zip:            \n",
    "            loaded_zip.extractall(raw_dataset_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "download_data(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the csv files of the train, val and test splits, we encode all three as a unique pandas Dataframe to be able to better inspect it and manipulate it as a whole.\n",
    "The Dataframe `df` is structured as follows: \n",
    "- `claim`: the fact to verify \n",
    "- `evidence`: one of the possibly multiple sentences in the dataset which supports or refutes the `claim`\n",
    "- `id`: number associated to the fact to verify (different rows can have the same `id`)\n",
    "- `label`: wether the evidence REFUTES or SUPPORTS the claim\n",
    "- `split`: the split to which one claim belongs (train, val, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the entire dataset in a pandas dataframe and add the split column\n",
    "def encode_dataset(): \n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for split in ['train','val','test']:\n",
    "        split_path = os.path.join(raw_dataset_path,f\"{split}_pairs.csv\")\n",
    "        split_df = pd.read_csv(split_path,index_col=0)\n",
    "        split_df['split'] = split\n",
    "\n",
    "        df = df.append(split_df,ignore_index=True,)\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df \n",
    "\n",
    "df = encode_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the newly created dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris Hemsworth appeared in A Perfect Getaway.</td>\n",
       "      <td>2\\tHemsworth has also appeared in the science ...</td>\n",
       "      <td>3</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roald Dahl is a writer.</td>\n",
       "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Roald Dahl is a governor.</td>\n",
       "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...</td>\n",
       "      <td>8</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ireland has relatively low-lying mountains.</td>\n",
       "      <td>10\\tThe island 's geography comprises relative...</td>\n",
       "      <td>9</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>10\\tThe island 's geography comprises relative...</td>\n",
       "      <td>10</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0     Chris Hemsworth appeared in A Perfect Getaway.   \n",
       "1                            Roald Dahl is a writer.   \n",
       "2                          Roald Dahl is a governor.   \n",
       "3        Ireland has relatively low-lying mountains.   \n",
       "4  Ireland does not have relatively low-lying mou...   \n",
       "\n",
       "                                            evidence  id     label  split  \n",
       "0  2\\tHemsworth has also appeared in the science ...   3  SUPPORTS  train  \n",
       "1  0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...   7  SUPPORTS  train  \n",
       "2  0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...   8   REFUTES  train  \n",
       "3  10\\tThe island 's geography comprises relative...   9  SUPPORTS  train  \n",
       "4  10\\tThe island 's geography comprises relative...  10   REFUTES  train  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The splits present in the dataframe are: ['train' 'val' 'test']\n",
      "Unique labels in the dataset: ['SUPPORTS' 'REFUTES']\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "print('The splits present in the dataframe are:',df['split'].unique())\n",
    "print('Unique labels in the dataset:',df['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can see that the dataset has been structured correctly.\\\n",
    "Now we print some values to check the dimensions of the different splits and to retrive useful informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (136094, 5)\n",
      "Number of example in train: 121740\n",
      "Number of example in val: 7165\n",
      "Number of example in test: 7189\n"
     ]
    }
   ],
   "source": [
    "print('Dataframe shape:', df.shape)\n",
    "print('Number of example in train:',len(df[df['split']=='train']))\n",
    "print('Number of example in val:',len(df[df['split']=='val']))\n",
    "print('Number of example in test:',len(df[df['split']=='test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of claims in the training split of the dataset is clearly much higher than that of val and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should probably undergo some preprocessing before it can be used to train our model. Even if this was already noticeable from the few examples taken from the dataframe that we printed above, let's now show an examples of an evidence to make more evident the work that we will need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1\\tAn ethnic Hungarian , she was born and raised in Novi Sad , SFR Yugoslavia .\\tNovi Sad\\tNovi Sad\\tSFR Yugoslavia\\tSFR Yugoslavia\\tHungarian\\tHungarians']\n"
     ]
    }
   ],
   "source": [
    "print(list(df.sample(1)['evidence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOH FORSE QUI TROVARE UN MODO PER VEDERE COSA ANDREBBE PULITO DAL DATASET "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both claims and evidence contain a lot of unwanted text: punctuation, symbols, meta-characters, foreign words, tags, ecc. For some reason claims are much cleaner that evidences. Nonetheless we will preprocess both of them, to end up with a more manageable and digestible text. Especially since all the unwanted text do not contribute to the general meaning of each sentence, which is what we are interested in.\n",
    "Our preprocessing pipeling will:\n",
    "- drop everything before the first '\\t' (every evidence seems to start with a number followed by '\\t')\n",
    "- delete all unnecessary spaces; only one space between each word will be left `QUESTO COMPRENDE \\n \\t \\s ?` \n",
    "- remove all tabs and newlines characters (there are many '\\t' in the dataset)  `???? MI SA CHE NON SERVE`\n",
    "- remove the rounded parenthesis (-LRB- and -RRB-)\n",
    "- drop words inside square brackets (everything that falls between -LSB- and -RSB-)\n",
    "- delete all words that contains non-english/non-numerical characters  (there are some greek letters for instance)\n",
    "- remove 's `E COSE SIMILI DA DEFINIRE O MAGARI NO`\n",
    "- drop everything after the last dot character (after that there are often some other words similar to tags which may be image descriptions or hyperlinks)\n",
    "- remove punctuation\n",
    "- set everything to lowercase\n",
    "- convert string in list of words \n",
    "\n",
    "`CAMBIARE L'ORDINE e AGGIUNGERNE ALTRE (la pipeline che c'è adesso è solo per andare avanti, possibili altre cose con nltk tipo stopwords, stemming, ecc `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def preprocess_pipeline(sentence:str):\n",
    "    \n",
    "    #drop everything before the first '\\t' \n",
    "    sentence = sentence[sentence.find('\\t')+1:]\n",
    "\n",
    "    #drop everything after the last period\n",
    "    period_idx = sentence.rfind('.')\n",
    "    if period_idx!= -1:\n",
    "        sentence = sentence[:period_idx]\n",
    "\n",
    "    #remove all rounded parenthesis \n",
    "    sentence = sentence.replace('-LRB-','').replace('-RRB-','')\n",
    "\n",
    "    #remove words inside square brackets\n",
    "    sentence = re.sub(\"-LSB.*?-RSB-\",\"\",sentence)\n",
    "\n",
    "    #remove all punctuation\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    #put everything to lowercase\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    #remove all unnecessary spaces and return a list of words\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our preprocessing pipeling we will apply it to an example in the dataset that we have identified to be a pretty tough one in terms of amount of cleanup necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original claim: Greece is a European country.\n",
      "Processed claim: ['greece', 'is', 'a', 'european', 'country']\n",
      "\n",
      "Original evidence: 0\tGreece -LRB- Ελλάδα , -LSB- eˈlaða -RSB- -RRB- , officially the Hellenic Republic -LRB- Greek : Ελληνική Δημοκρατία , -LSB- eliniˈci ðimokraˈti.a -RSB- -RRB- , historically also known as Hellas -LRB- Ἑλλάς , -LSB- heˈlas -RSB- , modern pronunciation Ellás -RRB- , is a country in southeastern Europe , with a population of approximately 11 million as of 2015 .\tGreek\tGreek people\tsoutheastern Europe\tsoutheastern Europe\n",
      "Processed evidence: ['greece', 'ελλάδα', 'officially', 'the', 'hellenic', 'republic', 'greek', 'ελληνική', 'δημοκρατία', 'historically', 'also', 'known', 'as', 'hellas', 'ἑλλάς', 'modern', 'pronunciation', 'ellás', 'is', 'a', 'country', 'in', 'southeastern', 'europe', 'with', 'a', 'population', 'of', 'approximately', '11', 'million', 'as', 'of', '2015']\n"
     ]
    }
   ],
   "source": [
    "#retrive from the dataset the 13th example. It is one about Greece in which the text is pretty messy \n",
    "original_claim = df.loc[13,'claim']\n",
    "original_evidence = df.loc[13,'evidence']\n",
    "\n",
    "processed_claim = preprocess_pipeline(original_claim)\n",
    "processed_evidence = preprocess_pipeline(original_evidence)\n",
    "\n",
    "print('Original claim:',original_claim)\n",
    "print('Processed claim:',processed_claim)\n",
    "print('\\nOriginal evidence:',original_evidence)\n",
    "print('Processed evidence:',processed_evidence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the final results relative to both the claim and evidence after the preprocessing are satisfacory. For this reason we are now going to apply the preprocessing function to the entire dataset encoded as a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[chris, hemsworth, appeared, in, a, perfect, g...</td>\n",
       "      <td>[hemsworth, has, also, appeared, in, the, scie...</td>\n",
       "      <td>3</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[roald, dahl, is, a, writer]</td>\n",
       "      <td>[roald, dahl, 13, september, 1916, 23, novembe...</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[roald, dahl, is, a, governor]</td>\n",
       "      <td>[roald, dahl, 13, september, 1916, 23, novembe...</td>\n",
       "      <td>8</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ireland, has, relatively, lowlying, mountains]</td>\n",
       "      <td>[the, island, s, geography, comprises, relativ...</td>\n",
       "      <td>9</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ireland, does, not, have, relatively, lowlyin...</td>\n",
       "      <td>[the, island, s, geography, comprises, relativ...</td>\n",
       "      <td>10</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[there, have, been, many, notable, performance...</td>\n",
       "      <td>[his, most, commercially, successful, role, to...</td>\n",
       "      <td>14</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[edward, i, of, england, responded, to, a, sec...</td>\n",
       "      <td>[after, suppressing, a, minor, rebellion, in, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[h, h, holmes, owned, a, building, west, of, c...</td>\n",
       "      <td>[many, victims, were, said, to, have, been, ki...</td>\n",
       "      <td>19</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[h, h, holmes, was, the, owner, of, a, buildin...</td>\n",
       "      <td>[many, victims, were, said, to, have, been, ki...</td>\n",
       "      <td>20</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[the, beastie, boys, released, pauls, boutique]</td>\n",
       "      <td>[in, 2009, they, released, digitally, remaster...</td>\n",
       "      <td>21</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  [chris, hemsworth, appeared, in, a, perfect, g...   \n",
       "1                       [roald, dahl, is, a, writer]   \n",
       "2                     [roald, dahl, is, a, governor]   \n",
       "3    [ireland, has, relatively, lowlying, mountains]   \n",
       "4  [ireland, does, not, have, relatively, lowlyin...   \n",
       "5  [there, have, been, many, notable, performance...   \n",
       "6  [edward, i, of, england, responded, to, a, sec...   \n",
       "7  [h, h, holmes, owned, a, building, west, of, c...   \n",
       "8  [h, h, holmes, was, the, owner, of, a, buildin...   \n",
       "9    [the, beastie, boys, released, pauls, boutique]   \n",
       "\n",
       "                                            evidence  id     label  split  \n",
       "0  [hemsworth, has, also, appeared, in, the, scie...   3  SUPPORTS  train  \n",
       "1  [roald, dahl, 13, september, 1916, 23, novembe...   7  SUPPORTS  train  \n",
       "2  [roald, dahl, 13, september, 1916, 23, novembe...   8   REFUTES  train  \n",
       "3  [the, island, s, geography, comprises, relativ...   9  SUPPORTS  train  \n",
       "4  [the, island, s, geography, comprises, relativ...  10   REFUTES  train  \n",
       "5  [his, most, commercially, successful, role, to...  14  SUPPORTS  train  \n",
       "6  [after, suppressing, a, minor, rebellion, in, ...  17  SUPPORTS  train  \n",
       "7  [many, victims, were, said, to, have, been, ki...  19  SUPPORTS  train  \n",
       "8  [many, victims, were, said, to, have, been, ki...  20  SUPPORTS  train  \n",
       "9  [in, 2009, they, released, digitally, remaster...  21  SUPPORTS  train  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['claim'] = df['claim'].apply(preprocess_pipeline)\n",
    "df['evidence'] = df['evidence'].apply(preprocess_pipeline)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to build the dictionaries that will be used for the numerical tokenization of the dataset and for the generation of the embedding matrix.\n",
    "\n",
    "The function `build_vocab` takes in input the list of unique words in the whole dataset and creates:\n",
    "- `word2int`: dictionary which associates each word with an integer.\n",
    "- `int2word`: dictionary which associates each integer with the relative word.\n",
    "\n",
    "These two dictionaries constitute a bijective mapping between words and indexes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = namedtuple('Vocabulary',['word2int','int2word','unique_words'])\n",
    "\n",
    "def build_vocab(unique_words : list[str]): \n",
    "    \"\"\"\n",
    "        Builds 4 dictionaries word2int, int2word and put them in the Vocabulary\n",
    "    \"\"\"\n",
    "    \n",
    "    word2int = OrderedDict()\n",
    "    int2word = OrderedDict()\n",
    "\n",
    "    for i, word in enumerate(unique_words):\n",
    "        word2int[word] = i+1           #plus 1 since the 0 will be used as tag token \n",
    "        int2word[i+1] = word\n",
    "    \n",
    "    return Vocab(word2int,int2word,unique_words)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b10856870a6af5fec4ffddd4d7318a6f2add2c9f3b4bd7caecf75cea33b7bd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
