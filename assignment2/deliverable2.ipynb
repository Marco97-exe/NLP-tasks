{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Fact checking, Neural Languange Inference (NLI)_**\n",
    "\n",
    "**Authors**: Giacomo Berselli, Marco Cucè, Riccardo De Matteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print all output for a cell instead of only last one \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import random\n",
    "import string \n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "import time \n",
    "import logging\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "# Fix data seed to achieve reproducible results\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current work directory: {}\".format(os.getcwd())) #print the current working directory \n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\") # directory containing the notebook\n",
    "\n",
    "if not os.path.exists(data_folder):   #create folder where all data will be stored \n",
    "    os.makedirs(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing first, we download the raw dataset, unzip it and store the csv document of each split in the dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_path = os.path.join(data_folder,'raw_dataset')   #path of the raw dataset as downloaded \n",
    "\n",
    "def save_response_content(response, destination):    \n",
    "    CHUNK_SIZE =32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks                \n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(data_folder):\n",
    "    zip_dataset_path = os.path.join(raw_dataset_path,'fever_data.zip')    \n",
    "    data_url_id =\"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"    \n",
    "    url =\"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(raw_dataset_path):        \n",
    "        os.makedirs(raw_dataset_path)\n",
    "\n",
    "    if not os.path.exists(zip_dataset_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:           \n",
    "            response = current_session.get(url, params={'id': data_url_id}, stream=True)\n",
    "\n",
    "        save_response_content(response, zip_dataset_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_dataset_path) as loaded_zip:            \n",
    "            loaded_zip.extractall(raw_dataset_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "download_data(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the csv files of the train, val and test splits, we encode all three as a unique pandas Dataframe to be able to better inspect it and manipulate it as a whole.\n",
    "The Dataframe `df` is structured as follows: \n",
    "- `claim`: the fact to verify \n",
    "- `evidence`: one of the possibly multiple sentences in the dataset which supports or refutes the `claim`\n",
    "- `id`: number associated to the fact to verify (different rows can have the same `id`)\n",
    "- `label`: wether the evidence REFUTES or SUPPORTS the claim\n",
    "- `split`: the split to which one claim belongs (train, val, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the entire dataset in a pandas dataframe and add the split column\n",
    "def encode_dataset(): \n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for split in ['train','val','test']:\n",
    "        split_path = os.path.join(raw_dataset_path,f\"{split}_pairs.csv\")\n",
    "        split_df = pd.read_csv(split_path,index_col=0)\n",
    "        split_df['split'] = split\n",
    "\n",
    "        df = df.append(split_df,ignore_index=True,)\n",
    "\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df \n",
    "\n",
    "df = encode_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the newly created dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "print('The splits present in the dataframe are:',df['split'].unique())\n",
    "print('Unique labels in the dataset:',df['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can see that the dataset has been structured correctly.\\\n",
    "Now we print some values to check the dimensions of the different splits and to retrive useful informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataframe shape:', df.shape)\n",
    "print('Number of example in train:',len(df[df['split']=='train']))\n",
    "print('Number of example in val:',len(df[df['split']=='val']))\n",
    "print('Number of example in test:',len(df[df['split']=='test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of claims in the training split of the dataset is clearly much higher than that of val and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset should probably undergo some preprocessing before it can be used to train our model. Even if this was already noticeable from the few examples taken from the dataframe that we printed above, let's now show an examples of an evidence to make more evident the work that we will need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df.sample(1)['evidence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOH FORSE QUI TROVARE UN MODO PER VEDERE COSA ANDREBBE PULITO DAL DATASET "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both claims and evidence contain a lot of unwanted text: punctuation, symbols, meta-characters, foreign words, tags, ecc. For some reason claims are much cleaner that evidences. Nonetheless we will preprocess both of them, to end up with a more manageable and digestible text. Especially since all the unwanted text do not contribute to the general meaning of each sentence, which is what we are interested in.\n",
    "Our preprocessing pipeling will:\n",
    "- drop everything before the first '\\t' (every evidence seems to start with a number followed by '\\t')\n",
    "- delete all unnecessary spaces; only one space between each word will be left `QUESTO COMPRENDE \\n \\t \\s ?` \n",
    "- remove all tabs and newlines characters (there are many '\\t' in the dataset)  `???? MI SA CHE NON SERVE`\n",
    "- remove the rounded parenthesis (-LRB- and -RRB-)\n",
    "- drop words inside square brackets (everything that falls between -LSB- and -RSB-)\n",
    "- delete all words that contains non-english/non-numerical characters  (there are some greek letters for instance)\n",
    "- remove 's `E COSE SIMILI DA DEFINIRE O MAGARI NO`\n",
    "- drop everything after the last dot character (after that there are often some other words similar to tags which may be image descriptions or hyperlinks)\n",
    "- remove punctuation\n",
    "- set everything to lowercase\n",
    "- convert string in list of words \n",
    "\n",
    "`CAMBIARE L'ORDINE e AGGIUNGERNE ALTRE (la pipeline che c'è adesso è solo per andare avanti, possibili altre cose con nltk tipo stopwords, stemming, ecc `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "# import unidecode\n",
    "# import unicodedata2\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# def lemmatize_and_remove_non_ascii(sentence:str):\n",
    "#     \"\"\"Remove unnecessary spaces, remove words with non ASCII characters and lemmatize\"\"\"\n",
    "#     sentence = sentence.split() #remove all unnecessary spaces and return a list of words\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     sentence = [lemmatizer.lemmatize(word) for word in sentence if word.isascii()] #if a word has all ASCII characters: lemmatize, else: remove\n",
    "#     return sentence\n",
    "\n",
    "# def stemm_and_remove_non_ascii(sentence: str):\n",
    "#     sentence = sentence.split() #remove all unnecessary spaces and return a list of words\n",
    "#     ps = PorterStemmer()\n",
    "#     sentence = [ps.stem(word) for word in sentence if word.isascii()]#if a word has all ASCII characters: stemm, else: remove\n",
    "#     return sentence\n",
    "\n",
    "# def remove_accents(text):\n",
    "#     \"\"\"Replace accentuated characters by their non-accentuated counterparts\"\"\"\n",
    "#     text = unicodedata2.normalize('NFKD', text)\n",
    "#     return \"\".join([c for c in text if not unicodedata2.combining(c)])\n",
    "\n",
    "def preprocess_pipeline(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing\"\"\"\n",
    "    \n",
    "    #drop everything before the first '\\t' \n",
    "    sentence = sentence[sentence.find('\\t')+1:]\n",
    "\n",
    "    #drop everything after the last period\n",
    "    period_idx = sentence.rfind('.')\n",
    "    if period_idx!= -1:\n",
    "        sentence = sentence[:period_idx]\n",
    "\n",
    "    #remove all rounded parenthesis \n",
    "    sentence = sentence.replace('-LRB-','').replace('-RRB-','')\n",
    "\n",
    "    #remove words inside square brackets\n",
    "    sentence = re.sub(\"-LSB.*?-RSB-\",\"\",sentence)\n",
    "\n",
    "    #remove all square brackets\n",
    "    sentence = sentence.replace('-LSB-','').replace('-RSB-','')\n",
    "\n",
    "    #remove all punctuation\n",
    "    sentence = sentence.translate(str.maketrans(dict.fromkeys(string.punctuation,' ')))\n",
    "\n",
    "    #subsitute the character ˈ with a space \n",
    "    sentence = sentence.replace('ˈ',' ')\n",
    "\n",
    "    #put everything to lowercase\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess_type1(sentence:str):\n",
    "    \"\"\"Apply standard preprocessing and return a list of words\"\"\"\n",
    "\n",
    "    sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "    #remove all unnecessary spaces and return a list of words\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "# def preprocess_type2(sentence:str):\n",
    "#     \"\"\"Apply standard preprocessing, remove accents, remove words \n",
    "#     with non ASCII characters, lemmatize and return a list of words\"\"\"\n",
    "\n",
    "#     sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "#     #replace accentuated characters by their non-accentuated counterparts\n",
    "#     sentence = remove_accents(sentence)\n",
    "\n",
    "#     #remove non-ascii words\n",
    "#     sentence = lemmatize_and_remove_non_ascii(sentence)\n",
    "\n",
    "#     return sentence\n",
    "\n",
    "# def preprocess_type3(sentence:str):\n",
    "#     \"\"\"Apply standard preprocessing, transliterates UNICODE characters in ASCII, \n",
    "#     remove words with non ASCII characters, lemmatize and return a list of words\"\"\"\n",
    "\n",
    "#     sentence = preprocess_pipeline(sentence)\n",
    "\n",
    "#     #transliterates any UNICODE string into the closest possible representation in ASCII text\n",
    "#     sentence = unidecode.unidecode(sentence)\n",
    "\n",
    "#     #remove non-ascii words\n",
    "#     sentence = lemmatize_and_remove_non_ascii(sentence)\n",
    "\n",
    "#     return sentence\n",
    "\n",
    "# def preprocess_type4(sentence: str):\n",
    "#     \"\"\"\n",
    "#         Apply standard preprocessing, removes stop-words and non ascii's,  and lemmatizes.\n",
    "#     \"\"\"\n",
    "\n",
    "#     sentence = preprocess_pipeline(sentence)\n",
    "#     sentence = unidecode.unidecode(sentence)\n",
    "#     lemmatized = lemmatize_and_remove_non_ascii(sentence)\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     filter_stop_words = [word for word in lemmatized if not word in stop_words]\n",
    "#     return filter_stop_words\n",
    "\n",
    "# def preprocess_type5(sentence: str):\n",
    "#     \"\"\"\n",
    "#         Apply standard preprocessing, removes non ascii's and stemmes.\n",
    "#     \"\"\"\n",
    "#     sentence = preprocess_pipeline(sentence)\n",
    "#     sentence = unidecode.unidecode(sentence)\n",
    "#     stemmed = stemm_and_remove_non_ascii(sentence)\n",
    "#     return stemmed\n",
    "\n",
    "# def preprocess_type6(sentence: str):\n",
    "#     \"\"\"\n",
    "#         Apply standard preprocessing, removes stop-words and non ascii's,  and stemmes.\n",
    "#     \"\"\"\n",
    "#     sentence = preprocess_pipeline(sentence)\n",
    "#     sentence = unidecode.unidecode(sentence)\n",
    "#     stemmed = stemm_and_remove_non_ascii(sentence)\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     filter_stop_words = [word for word in stemmed if not word in stop_words]\n",
    "#     return filter_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our preprocessing pipeline we will apply it to an example in the dataset that we have identified to be a pretty tough one in terms of amount of cleanup necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive from the dataset the 13th example. It is one about Greece in which the text is pretty messy \n",
    "\n",
    "original_claim = df.loc[0,'claim']\n",
    "original_evidence = df.loc[0,'evidence']\n",
    "\n",
    "processed_claim = preprocess_type1(original_claim)\n",
    "processed_evidence = preprocess_type1(original_evidence)\n",
    "\n",
    "print('Original claim:',original_claim)\n",
    "print('Processed claim:',processed_claim,'\\n')\n",
    "print('Original evidence:',original_evidence)\n",
    "print('Processed evidence:',processed_evidence,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the final results relative to both the claim and evidence after the preprocessing are satisfacory. For this reason we are now going to apply the preprocessing function to the entire dataset encoded as a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['claim'] = df['claim'].apply(preprocess_type1)\n",
    "df['evidence'] = df['evidence'].apply(preprocess_type1)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to build the dictionaries that will be used for the numerical tokenization of the dataset and for the generation of the embedding matrix.\n",
    "\n",
    "The function `build_vocab` takes in input the list of unique words in the whole dataset and creates:\n",
    "- `word2int`: dictionary which associates each word with an integer.\n",
    "- `int2word`: dictionary which associates each integer with the relative word.\n",
    "\n",
    "These two dictionaries constitute a bijective mapping between words and indexes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = namedtuple('Vocabulary',['word2int','int2word','unique_words'])\n",
    "\n",
    "def build_vocab(unique_words : list[str]): \n",
    "    \"\"\"\n",
    "        Builds the dictionaries word2int, int2word and put them in the Vocabulary\n",
    "    \"\"\"\n",
    "    word2int = OrderedDict()\n",
    "    int2word = OrderedDict()\n",
    "\n",
    "    for i, word in enumerate(unique_words):\n",
    "        word2int[word] = i+1           #plus 1 since the 0 will be used as tag token \n",
    "        int2word[i+1] = word\n",
    "    \n",
    "    return Vocab(word2int,int2word,unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `build_vocab` needs in input the list of all the unique words in the dataset, so we're now going to retrive it from the dataset to be able to build the dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_claim = df['claim'].explode().unique().tolist()  \n",
    "unique_words_evidence = df['evidence'].explode().unique().tolist()\n",
    "\n",
    "print('the number of unique words belonging to claims is:', len(unique_words_claim))\n",
    "print('the number of unique words belonging to evidences is:', len(unique_words_evidence))\n",
    "\n",
    "unique_words = set(unique_words_evidence + unique_words_claim)\n",
    "print('the number of unique words in the entire dataset is:', len(unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the vocabulary which contains the mapping between word and index we can 'numberise' the dataset. In particular we will add to the Dataframe 3 columns:\n",
    "- `idx_claim`: same as `claim` but with each word substituted by its index.\n",
    "- `idx_evidence`: same as `evidence` but with each word substituted by its index.\n",
    "- `idx_label`: label encoding as a unique integer (0 or 1 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indexed_dataframe(df: pd.DataFrame):\n",
    "\n",
    "    df['idx_claim'] = df.claim.apply(lambda x:list(map(vocab.word2int.get,x)))\n",
    "    df['idx_evidence'] = df.evidence.apply(lambda x:list(map(vocab.word2int.get,x)))\n",
    "\n",
    "    df['label'] = df.label.astype('category')   #convert the label column into category dtype\n",
    "    df['idx_label'] = df.label.cat.codes        #assign unique integer to each category\n",
    "\n",
    "    return df \n",
    "\n",
    "def check_dataframe_numberization(df,vocab):\n",
    "\n",
    "    \"\"\"\n",
    "       Checks if the numberized dataframe will lead to the normal dataframe usind the reverse mapping \n",
    "    \"\"\"\n",
    "\n",
    "    claims = df['claim']\n",
    "    evidences = df['evidence']\n",
    "\n",
    "    idx_to_claims = df.idx_claim.apply(lambda x:list(map(vocab.int2word.get,x)))\n",
    "    idx_to_evidences = df.idx_evidence.apply(lambda x:list(map(vocab.int2word.get,x)))\n",
    "\n",
    "    if claims.equals(idx_to_claims) and evidences.equals(idx_to_evidences):\n",
    "        print('All right with dataset numberization')\n",
    "    else:\n",
    "        raise Exception('There are problems with Dataset numberization')\n",
    "\n",
    "df = build_indexed_dataframe(df)\n",
    "\n",
    "check_dataframe_numberization(df,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the operation was successful, let's have a look at the numebrized dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Loaders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate mini-batches for each split to be passed to the network we leveraged a `torchtext` utility, such as `BucketIterator`. It ensures that each mini-batch is composed of sequences of nearly the same length (depending on the chosen batch size), in order to add the minimum padding possible to each Tensor. In order to do so, we needed to create a Pytorch Dataset since this is what is requested by the BucketIterator.\\\n",
    "The problem now is how to define the length of the input to the model (which is used to create buckets of similar-lenghts sequences), since for this task we are dealing with multiple inputs (claim and evidence). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_len = df.claim.apply(len)\n",
    "evidence_len = df.evidence.apply(len)\n",
    "print('average length of a claim sentence:',claim_len.mean())\n",
    "print('average length of a evidence sentence:',evidence_len.mean())\n",
    "print('max difference in length of claim sentences:',claim_len.max() - claim_len.min())\n",
    "print('max difference in length of evidence sentences:',evidence_len.max() - evidence_len.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the fact that the average sentence length for an evidence is much bigger than for a claim, we decided to create buckets based on the length of the evidence and only with that being equal, based on the claim's length. So the minibatches will be constructed by grouping similar-size evidences and their corresponing claims.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import BucketIterator\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataframeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "\n",
    "        dataframe = dataframe.copy()\n",
    "        self.claims = dataframe['idx_claim']      #column of numberized claims \n",
    "        self.evidences = dataframe['idx_evidence']   #column of numberized evidences \n",
    "        self.labels = dataframe['idx_label']       #column of categorical label \n",
    "        self.claim_ids = dataframe['id']          #column of claim ids \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claims)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'claim': self.claims[idx],\n",
    "                'evidence': self.evidences[idx],\n",
    "                'label': self.labels[idx],\n",
    "                'claim_id': self.claim_ids[idx]}\n",
    "\n",
    "def create_dataloaders(b_s : int, dataframe: pd.DataFrame):     #b_s = batch_size\n",
    "    \n",
    "    train_df = dataframe[dataframe['split'] == 'train'].reset_index(drop=True)      \n",
    "    val_df = dataframe[dataframe['split'] == 'val'].reset_index(drop=True)\n",
    "    test_df = dataframe[dataframe['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "    #create DataframeDataset objects for each split \n",
    "    train_dataset = DataframeDataset(train_df)\n",
    "    val_dataset = DataframeDataset(val_df)\n",
    "    test_dataset = DataframeDataset(test_df)\n",
    "\n",
    "\n",
    "    # Group similar length text sequences together in batches and return an iterator for each split.\n",
    "    train_dataloader,val_dataloader,test_dataloader = BucketIterator.splits((train_dataset,val_dataset,test_dataset),\n",
    "                                                        batch_sizes=(b_s,b_s,b_s), sort_key=lambda x: (len(x['evidence']),len(x['claim'])), \n",
    "                                                        repeat=True, sort=False, shuffle=True, sort_within_batch=True)\n",
    "    \n",
    "    return train_dataloader,val_dataloader,test_dataloader \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that the dataloaders have been created correctly. In order to do that, we print the indexed claim, the indexed evidence and the claim id of the first element of the train dataloader's first batch, and then we search it in the dataframe to see if the indexes correspond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch_size = 128\n",
    "tr, vl, ts = create_dataloaders(temp_batch_size, df)\n",
    "random_idx = random.randint(0, temp_batch_size-1)\n",
    "tr.init_epoch()\n",
    "for batch_id, batch in enumerate(tr.batches):\n",
    "    print(\"Claim: \", batch[random_idx]['claim'])\n",
    "    print(\"Evidence: \", batch[random_idx]['evidence'])\n",
    "    print(\"Label: \", batch[random_idx]['label'])\n",
    "    print(\"Claim id: \", batch[random_idx]['claim_id'], \"\\n\")\n",
    "    print(\"Corresponding row in the dataset:\")\n",
    "    df[df['id'] == (batch[random_idx]['claim_id'])]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally build an embedding matrix that will be used by the embedding layer of our model to store pre-trained word embeddings and retrive them using indices. \n",
    "The function `build_embedding_matrix`, via the passed embedding model and the `word2int` dictionary, costructs a matrix that stores at each word-index the corresponding embedding vector found in GloVe. In particular we decided to use Glove as embedding model with a vector dimension of 300. \n",
    " \n",
    "In order to handle OOV words:\n",
    "- If a word in the dataset (identified by its unique integer) is present in GloVe model, we store its embedding vector in the embedding matrix.\n",
    "- Otherwise we assign as embedding to the OOV word a random vector of size 300, sampled from a uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing first, we need to download the `GloVe model` from gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix_path = os.path.join(data_folder, \"emb_matrix.npy\")\n",
    "\n",
    "def download_glove_emb(force_download = False):   \n",
    "    \"\"\"\n",
    "        Download the glove embedding model and returns it \n",
    "    \"\"\"\n",
    "    emb_model = None\n",
    "\n",
    "    if os.path.exists(emb_matrix_path) and not force_download: \n",
    "        print('embedding matrix already saved in data folder')\n",
    "\n",
    "    else:\n",
    "        print('downloading glove embeddings ')        \n",
    "        embedding_dimension=300\n",
    "\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "        emb_model = gloader.load(download_path)\n",
    "        \n",
    "    return emb_model\n",
    "\n",
    "force_download = False      # to download glove model even if the emb_matrix has been already create. Mainly for testing purposes\n",
    "\n",
    "glove_embeddings = download_glove_emb(force_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the glove embeddings, we can check if there are some Out Of Vocabulary (OOV) words in our processed dataset.\n",
    "\\\n",
    "A word is considered OOV if it is present in our dataset but not in the GloVe embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, vocab):\n",
    "    \"\"\"\n",
    "        Given the embedding model and the unique words in the dataframe, determines the out-of-vocabulary words \n",
    "    \"\"\"\n",
    "    oov_words = []\n",
    "    idx_oov_words = []\n",
    "\n",
    "    if embedding_model is None:\n",
    "        print('WARNING: empty model, remember to download GloVe first or set force_dowload to True')\n",
    "\n",
    "    else: \n",
    "        for word in vocab.unique_words:\n",
    "            try: \n",
    "                embedding_model[word]\n",
    "            except:\n",
    "                oov_words.append(word) \n",
    "                idx_oov_words.append(vocab.word2int[word]) \n",
    "        \n",
    "        print(\"Total number of unique words in dataset:\",len(vocab.unique_words))\n",
    "        print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(oov_words), (float(len(oov_words)) / len(vocab.unique_words))*100))\n",
    "        print(\"Some OOV terms:\",random.sample(oov_words,15))\n",
    "    \n",
    "    return oov_words, idx_oov_words\n",
    "\n",
    "oov_words, idx_oov_words = check_OOV_terms(glove_embeddings,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed dataset contains a total number of 35096 unique words. By using the GloVe embeddings for our embedding matrix, we obtain 3745 OOV words, that is the 8.97% of all different words in our dataset.\n",
    "\n",
    "Let's build the embedding matrix then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(emb_model: gensim.models.keyedvectors.KeyedVectors,vocab) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        If the embedding for the word is present, add it to the embedding_matrix, otherwise insert a vector of random values.\n",
    "        Return the embedding matrix\n",
    "    \"\"\"\n",
    "    if emb_model is None:\n",
    "        print('WARNING: empty model, remember to download GloVe first or set force_download to True')\n",
    "        return None\n",
    "\n",
    "    embedding_dimension = len(emb_model[0]) #how many numbers each emb vector is composed of                                                           \n",
    "    embedding_matrix = np.zeros((len(vocab.word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros \n",
    "\n",
    "    for word, idx in vocab.word2int.items():\n",
    "        try:\n",
    "            embedding_vector = emb_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector     #assign the retrived or the generated vector to the corresponding index \n",
    "    \n",
    "    print('Saving embedding matrix')\n",
    "    np.save(emb_matrix_path,embedding_matrix,allow_pickle=True)\n",
    "    print(\"Embedding matrix shape: {}\".format(embedding_matrix.shape))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = build_embedding_matrix(glove_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first few rows of the freshly created embedding matrix, to get a sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(embedding_matrix).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the very first row is full of zeros since that's a 'fake embedding' for the padding token which will never be used in practice.\n",
    "\n",
    "To be completely sure that the embedding matrix has been built correctly, we check that the embedding vector associated with an index in the embedding matrix is the same as the one retrieved from glove by passing to it the word to which that index correspond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_id_corr(glove: gensim.models.keyedvectors.KeyedVectors, vocab, matrix, dataframe):\n",
    "    \"\"\"\n",
    "        Checks whether the numberized dataframe and the index of the embedding matrix correspond\n",
    "    \"\"\"\n",
    "    if not glove:\n",
    "        print('WARNING: empty model, remember to download GloVe first or set force_dowload to True')\n",
    "        return \n",
    "    oov_words_ = []\n",
    "\n",
    "    for indexed_sentence in dataframe['idx_claim']+dataframe['idx_evidence']:\n",
    "\n",
    "        for token in indexed_sentence:\n",
    "            embedding = matrix[token]\n",
    "            word = vocab.int2word[token]\n",
    "            if word in glove.key_to_index:\n",
    "                assert(np.array_equal(embedding,glove[word]))\n",
    "            else:\n",
    "                oov_words_.append(word)\n",
    "\n",
    "    print('Double check OOV number:',len(set(oov_words_)))\n",
    "\n",
    "check_id_corr(glove_embeddings,vocab,embedding_matrix,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no error has been found, we can safely proceed with the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid downloading the GloVe embeddings more than once, since the process is really slow, in the case that this is not the first run and the embedding matrix has been already created and saved we can load it from the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "        Loads the saved emb_matrix from the data folder \n",
    "    \"\"\"\n",
    "    print('Loading embedding matrix')\n",
    "    emb_matrix = np.load(emb_matrix_path,allow_pickle=True)\n",
    "    print('Loaded')\n",
    "\n",
    "    return emb_matrix\n",
    "\n",
    "if os.path.exists(emb_matrix_path) :\n",
    "    embedding_matrix = load_data()\n",
    "else:\n",
    "    print('What you are looking for is not present in the folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytoch imports\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "#scikit-learn imports \n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "log = logging.getLogger('logger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use a dynamic approach, we define a single custom model which will build the correct architecture based on the paramaters passed to the tuple `Architecture`. In particular, the first parameter of the tuple stores the strategy chosen to embed the sentences of the claim and the evidence. You can choose between:\n",
    "- `mlp`: encode token sequences via a simple MLP layer.\n",
    "- `rnn_last`: encode token sequences via a RNN and take the last state as the sentence embedding.\n",
    "- `rnn_avg`: encode token sequences via a RNN and average all the output states.\n",
    "- `bag_of_vectors`: compute the sentence embedding as the mean of its token embeddings.\n",
    "\n",
    "The second parameter defines the technique chosen to merge evidence and claim sentence embeddings, as follows:\n",
    "- `concat`: define the classification input as the concatenation of evidence and claim sentence embeddings.\n",
    "- `sum`: define the classification input as the sum of evidence and claim sentence embeddings.\n",
    "- `mean`: define the classification input as the mean of evidence and claim sentence embeddings.\n",
    "\n",
    "The third parameter instead is a boolean and allows to add an additional feature, the cosine similarity. `cosine_sim` allows to see if some similarity information between the claim to verify and one of its associated evidence might be useful to the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Architecture = namedtuple('Architecture',['sentence_emb_strat','merge_input','cosine_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_model(nn.Module):\n",
    "    \"\"\"\n",
    "        Class defining our model architecture  \n",
    "    \"\"\"\n",
    "    def __init__(self, emb_matrix: np.ndarray, pad_idx: int, max_tokens: int, architecture_param : Architecture, device) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_tokens = max_tokens\n",
    "        self.pad_idx = pad_idx \n",
    "        self.device = device\n",
    "\n",
    "        self.param = architecture_param\n",
    "\n",
    "\n",
    "        self.embedding_layer, self.word_embedding_dim = self.build_emb_layer(emb_matrix,pad_idx)\n",
    "\n",
    "        if self.param.sentence_emb_strat == 'mlp':\n",
    "            self.mlp = nn.Linear(max_tokens,1)\n",
    "\n",
    "        elif self.param.sentence_emb_strat in ('rnn_last','rnn_avg'):\n",
    "            self.rnn = nn.LSTM(self.word_embedding_dim, self.word_embedding_dim, batch_first = True) \n",
    "        \n",
    "        #determine the input dimension of the last layer that will classify each claim \n",
    "        classifier_input_dim = int(self.param.cosine_sim) + (\n",
    "            self.word_embedding_dim * 2\n",
    "            if self.param.merge_input == \"concat\"\n",
    "            else self.word_embedding_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(classifier_input_dim,1)   #TODO : output dim da capire \n",
    "\n",
    "        self.to(self.device)  #move model to device , 'gpu' if possible \n",
    "        \n",
    "\n",
    "    \n",
    "    def build_emb_layer(self, weights_matrix: np.ndarray, pad_idx : int, freeze = True):\n",
    "    \n",
    "        matrix = torch.Tensor(weights_matrix).to(self.device)   #the embedding matrix \n",
    "        _ , embedding_dim = matrix.shape \n",
    "\n",
    "        emb_layer = nn.Embedding.from_pretrained(matrix, freeze=freeze, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable (TODO: trainable ? )\n",
    "        \n",
    "        return emb_layer, embedding_dim\n",
    "        \n",
    "\n",
    "    def pad_batch(self,batch: list):\n",
    "        \"\"\"\n",
    "            Input:  List of Tensors of variable length\n",
    "            Output: Batch of tensors all padded to the same length \n",
    "        \"\"\"\n",
    "        batch = batch.copy()\n",
    "        \n",
    "        #if we are going to use an mlp as sentence embedding strategy, all the sentences should be padded to max_tokens length\n",
    "        if self.param.sentence_emb_strat == 'mlp':\n",
    "            batch[0] = nn.ConstantPad1d((0,self.max_tokens-batch[0].shape[0]),0)(batch[0])  \n",
    "\n",
    "        padded_batch = rnn.pad_sequence(batch,batch_first = True, padding_value = self.pad_idx)\n",
    "\n",
    "        padded_batch = padded_batch.to(self.device)    #move tensor to gpu if possible \n",
    "\n",
    "        return padded_batch\n",
    "\n",
    "\n",
    "    def words_embedding(self, word_idxs):   #get embedding vectors for each token in sentence \n",
    "        \"\"\"\n",
    "            Input:  [batch_size, num_tokens]\n",
    "            Output: [batch_size, num_tokens, embedding_dim]\n",
    "        \"\"\"\n",
    "        return self.embedding_layer(word_idxs)\n",
    "    \n",
    "    def sentence_embedding(self, embeddings, sentence_lenghts):     #compute sentence embedding \n",
    "        \"\"\"\n",
    "            Input:  [batch_size, num_tokens, embedding_dim]\n",
    "            Output: [batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        strat = self.param.sentence_emb_strat\n",
    "\n",
    "        def mlp():  #compute sentence embedding via dense layer \n",
    "            \n",
    "            reshaped_embeddings = embeddings.permute(0,2,1)     #swap last two dimensions since Linear operates only on last dimension\n",
    "\n",
    "            sentence_emb = self.mlp(reshaped_embeddings)   \n",
    "\n",
    "            return sentence_emb.squeeze(2)   #remove dimension of size 1 \n",
    "        \n",
    "        def rnn_last():    #take as sentence embedding the last state of the rnn \n",
    "            \n",
    "            packed_embeddings = pack_padded_sequence(embeddings, sentence_lenghts.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "            # batch_size = embeddings.shape[0]\n",
    "            # h_0 = Variable(torch.zeros(1, batch_size, self.word_embedding_dim).cuda())   TODO : remove all \n",
    "            # c_0 = Variable(torch.zeros(1, batch_size, self.word_embedding_dim).cuda())\n",
    "\n",
    "            packed_out, (last_h, _)  = self.rnn(packed_embeddings)   #,(h_0, c_0)\n",
    "\n",
    "            if torch.isnan(last_h.squeeze(0)).any():    \n",
    "                log.debug('nan in last_h')  \n",
    "                raise Exception(' nan in last h ')\n",
    "            \n",
    "            return last_h.squeeze(0)  #remove first dimension of 1 (TODO: if bidirectional or more than 1 layer this has to be handled)\n",
    "        \n",
    "        def rnn_avg():   #take as sentence embedding the average of all the states of the rnn corresponing to each word \n",
    "\n",
    "            packed_embeddings = pack_padded_sequence(embeddings, sentence_lenghts.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            packed_out, _  = self.rnn(packed_embeddings)\n",
    "\n",
    "            unpacked_out, l = pad_packed_sequence(packed_out,batch_first=True)\n",
    "\n",
    "            avg = unpacked_out.sum(dim=1).div(sentence_lenghts.unsqueeze(dim=1))\n",
    "\n",
    "            return avg\n",
    "        \n",
    "        def bag_of_vectors():  #sentence embedding as the mean of its token embeddings\n",
    "\n",
    "            avg = embeddings.sum(dim=1).div(embeddings.count_nonzero(dim=1))\n",
    "\n",
    "            return avg \n",
    "\n",
    "        if strat == 'mlp':\n",
    "            return mlp()\n",
    "        elif strat == 'rnn_last':\n",
    "            return rnn_last()\n",
    "        elif strat == 'rnn_avg':\n",
    "            return rnn_avg()\n",
    "        elif strat == 'bag_of_vectors':\n",
    "            return bag_of_vectors()\n",
    "        else :\n",
    "            raise Exception('Incorrect name for sentence embedding strategy')\n",
    "\n",
    "    def merge_sentence_emb(self,claims,evidences):\n",
    "        \"\"\"\n",
    "            Input:  claims -> [batch_size, embedding_dim] , evidences -> [batch_size, embedding_dim]\n",
    "            Output: [batch_size, dim] dim is based on strategy specified \n",
    "        \"\"\"\n",
    "\n",
    "        strat = self.param.merge_input\n",
    "\n",
    "        if strat == 'concat':\n",
    "            result = torch.cat((claims,evidences),dim=1)      #concatenate the two tensors \n",
    "        elif strat == 'sum': \n",
    "            result = torch.stack((claims,evidences), dim=0).sum(dim=0)    #sum the two tensors \n",
    "        elif strat == 'mean':\n",
    "            result = torch.stack((claims,evidences),dim=0).mean(dim=0)    #compute mean of the two tensors \n",
    "        \n",
    "        if self.param.cosine_sim :\n",
    "            cosine_sim = F.cosine_similarity(claims,evidences).unsqueeze(-1)\n",
    "            result = torch.cat((result,cosine_sim),dim=1)                      #add to previously generated merged ouput, one value representing cosine similarity between the two input tensors \n",
    "\n",
    "        return result \n",
    "\n",
    "\n",
    "    def forward(self, claims, claim_lengths, evidences, evidence_lengths):\n",
    "\n",
    "        #pad the sentences to have fixed size \n",
    "        padded_claims = self.pad_batch(claims)\n",
    "        # print('padded_claims = ', padded_claims)\n",
    "        padded_evidences = self.pad_batch(evidences)\n",
    "        # print('padded_evidence = ', padded_evidences)\n",
    "        \n",
    "        #embed each word in a sentence with a 300d vector \n",
    "        word_emb_claims = self.words_embedding(padded_claims)       \n",
    "        # print('word_emb_claims = ', word_emb_claims)   \n",
    "        word_emb_evidences = self.words_embedding(padded_evidences)\n",
    "        # print('word_emb_evidences = ', word_emb_evidences)\n",
    "\n",
    "        #compute sentence embedding\n",
    "        sentence_emb_claims = self.sentence_embedding(word_emb_claims,claim_lengths)\n",
    "        # print('sentence_emb_claims = ', sentence_emb_claims)\n",
    "        sentence_emb_evidences = self.sentence_embedding(word_emb_evidences,evidence_lengths)\n",
    "        # print('sentence_emb_evidences = ', sentence_emb_evidences)\n",
    "\n",
    "        #merge multi-inputs \n",
    "        classification_input = self.merge_sentence_emb(sentence_emb_claims,sentence_emb_evidences)\n",
    "\n",
    "        #final classification \n",
    "        predictions = self.classifier(classification_input)\n",
    "\n",
    "        predictions = predictions.squeeze()   #remove dim of size 1 \n",
    "\n",
    "        return predictions \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we define the functions that will be used in the 'train & eval' and 'test' pipeline to compute the metrics of the models.\\\n",
    "In particular, there are two types of evaluations:\n",
    "- Multi-input classification evaluation: this type of evaluation is the easiest and concerns computing evaluation metrics, such as accuracy and f1-score of our pre-processed dataset. In other words, we assess the performance of chosen classifiers.\n",
    "- Claim verification evaluation: if we want to give an answer concerning the claim itself, we need to consider the whole evidence set. Intuitively, for a given claim, we consider all its corresponding (claim, evidence) pairs and  their corresponding classification outputs. At this point, all we need to do is to compute the final predicted claim label via majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute accuracy and f1-score \n",
    "def acc_and_f1(y_true: torch.LongTensor,y_pred: torch.LongTensor):\n",
    "    \"\"\"\n",
    "        Compute accuracy and f1-score for an epoch \n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true,y_pred,average='macro')\n",
    "\n",
    "    return acc,f1\n",
    "\n",
    "#construct y_true and y_pred lists to be passed to acc_and_f1 function, but based on majority voting strategy\n",
    "def majority_voting(y_true, y_pred, y_ids):\n",
    "    \"\"\"\n",
    "        Input: the list of corresponing true labels, the list of predicted labels, the list of claim ids to compute majority voting\n",
    "        Output : the list of true labels (one for each claim id), the list of predicted labels via majority voting\n",
    "\n",
    "        Idea behind the implementation: Since there could be more claims with the same id in the dataset, we start by counting the\n",
    "        number of occurrences of each claim id in the dataset and we store them sorted on the id number (tensor 'a'). \n",
    "        Then we count for each id how many times we predict it as supported in the predicted tensor 'y_pred', where a 'SUPPORTS' \n",
    "        prediction is considered as a 1 (integer), so in practice we sum the 1s founded and again we store the results sorted on the \n",
    "        id number (tensor 'b'). Next we create a tensor 'true', containing the true label for each claim and sorted on the id number \n",
    "        (so if the true label of the first claim is supported, the integer at index 0 will be a 1), Finally we create the tensor \n",
    "        'pred' which verifies for each element of 'b' (so for each number of SUPPORTS for each id), if it is greater than the number \n",
    "        of occurrences of that id in the dataset divided by 2. This means that, if the result is positive, we have predicted that \n",
    "        claim in most cases as supported, otherwise as refuted (same number of SUPPORTS and REFUTES will be considered as REFUTES), and \n",
    "        we store it again in a tensor sorted on the id number.\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    search_sorted = torch.searchsorted(y_ids.unique(),y_ids)\n",
    "    a = torch.bincount(search_sorted)           #number of occurrences for each id in the dataset\n",
    "    b = torch.bincount(search_sorted, y_pred)   #for each id how many 1s (SUPPORTS) there are in the predicted tensor\n",
    "    true = (torch.bincount(search_sorted, y_true) > 0).int()    #tensor (sorted on claim id) containing the true label for each claim (1: SUPPORTS, 0: REFUTES)\n",
    "    pred = (b > (a / 2)).int()  #tensor (sorted on claim id) containing the predicted label for each claim (1: SUPPORTS, 0: REFUTES)\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    log.debug('maj voting:',end-start)\n",
    "\n",
    "    return true, pred\n",
    "\n",
    "#compute accuracy and f1 score via majority voting \n",
    "def acc_and_f1_majority(y_true, y_pred, y_ids):\n",
    "\n",
    "    y_true, y_pred = majority_voting(y_true,y_pred,y_ids)\n",
    "\n",
    "    acc, f1 = acc_and_f1(y_true,y_pred)\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define the train and validation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model: Custom_model, iterator : BucketIterator, optimizer: optim.Optimizer, criterion, device):\n",
    "    \"\"\" Args:\n",
    "         - model: the model istantiated with pre-defined hyperparameters.\n",
    "         - iterator: dataloader for passing data to the network in batches \n",
    "         - optimizer: optimizer for backward pass \n",
    "         - criterion: loss function \n",
    "         - device: 'gpu' if it's available, 'cpu' otherwise \n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    batch_loss = 0\n",
    "    \n",
    "    #aggregate all the predictions and corresponding true labels (and claim ids) in tensors \n",
    "    all_pred , all_targ, all_ids = torch.LongTensor(), torch.LongTensor(), torch.LongTensor()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    iterator.init_epoch()  #generate and shuffles batches from dataloader #TODO: create_batches \n",
    "\n",
    "    for batch_id, batch in enumerate(iterator.batches):\n",
    "\n",
    "        claims_batch = [torch.LongTensor(example['claim']) for example in batch]    #list of tensors of words id for each sentence in a batch \n",
    "        evidences_batch = [torch.LongTensor(example['evidence']) for example in batch]     #list of tensors of tags id for each sentence in a batch \n",
    "\n",
    "        claims_lengths = torch.Tensor([len(example['claim']) for example in batch])         #lenght of each claim sentence before padding \n",
    "        evidence_lengths = torch.Tensor([len(example['evidence']) for example in batch])         #lenght of each evidence sentence before padding \n",
    "\n",
    "        target_labels = torch.Tensor([example['label'] for example in batch])     #label of each example in a batch\n",
    "        target_ids = torch.LongTensor([example['claim_id'] for example in batch])  #id of each claim in a batch \n",
    "\n",
    "        #move tensors to gpu if possible \n",
    "        claims_lengths = claims_lengths.to(device)\n",
    "        evidence_lengths = evidence_lengths.to(device)\n",
    "        target_labels = target_labels.to(device)    \n",
    "\n",
    "        #zero the gradients \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        optimizer.zero_grad()            \n",
    "\n",
    "        predictions = model(claims_batch,claims_lengths,evidences_batch,evidence_lengths)   #generate predictions \n",
    "\n",
    "        loss = criterion(predictions, target_labels)      #compute the loss \n",
    "\n",
    "\n",
    "        pred = (predictions > 0.0 ).int().cpu()              #get class label \n",
    "\n",
    "        #concatenate the new tensors with the one computed in previous steps\n",
    "        all_pred = torch.cat((all_pred,pred))          \n",
    "        all_targ = torch.cat((all_targ,target_labels.long().cpu()))\n",
    "        all_ids = torch.cat((all_ids,target_ids))\n",
    "\n",
    "        #backward pass \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "\n",
    "    acc, f1 = acc_and_f1(all_targ,all_pred)\n",
    "\n",
    "    maj_acc, maj_f1 = acc_and_f1_majority(all_targ,all_pred,all_ids)\n",
    "\n",
    "    loss = batch_loss/(batch_id+1)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    log.debug('train epoch time:',start-end)\n",
    "\n",
    "    return loss, acc, f1, maj_acc, maj_f1,predictions\n",
    "\n",
    "\n",
    "def eval_loop(model: Custom_model, iterator: BucketIterator, criterion, device):\n",
    "    \"\"\" Args:\n",
    "         - model: the sequence pos tagger model istantiated with fixed hyperparameters.\n",
    "         - iterator: dataloader for passing data to the network in batches \n",
    "         - criterion: loss function \n",
    "         - device: 'gpu' if it's available, 'cpu' otherwise \n",
    "    \"\"\"\n",
    "     \n",
    "    start = time.perf_counter()\n",
    "\n",
    "    batch_loss = 0\n",
    "    \n",
    "    all_pred , all_targ, all_ids = torch.LongTensor(), torch.LongTensor(), torch.LongTensor() \n",
    "    \n",
    "    model.eval()   #model in eval mode \n",
    "    \n",
    "    iterator.init_epoch()  #TODO create_batches \n",
    "\n",
    "    with torch.no_grad(): #without computing gradients since it is evaluation loop\n",
    "    \n",
    "        for batch_id, batch in enumerate(iterator.batches):\n",
    "            \n",
    "            claims_batch = [torch.LongTensor(example['claim']) for example in batch]    #list of tensors of words id for each sentence in a batch \n",
    "            evidences_batch = [torch.LongTensor(example['evidence']) for example in batch]     #list of tensors of tags id for each sentence in a batch \n",
    "\n",
    "            claims_lengths = torch.Tensor([len(example['claim']) for example in batch])         #lenght of each claim sentence before padding \n",
    "            evidence_lengths = torch.Tensor([len(example['evidence']) for example in batch])         #lenght of each evidence sentence before padding \n",
    "\n",
    "            target_labels = torch.Tensor([example['label'] for example in batch])     #label of each example in a batch\n",
    "            target_ids = torch.LongTensor([example['claim_id'] for example in batch])  #id of each claim in a batch \n",
    "\n",
    "            #move tensors to gpu if possible \n",
    "            claims_lengths = claims_lengths.to(device)\n",
    "            evidence_lengths = evidence_lengths.to(device)\n",
    "            target_labels = target_labels.to(device)    \n",
    "\n",
    "            \n",
    "            predictions = model(claims_batch,claims_lengths,evidences_batch,evidence_lengths)   #generate predictions \n",
    "\n",
    "            loss = criterion(predictions, target_labels)      #compute the loss \n",
    "            \n",
    "            pred = (predictions > 0.0 ).int().cpu()         #get class label \n",
    "\n",
    "            #concatenate the new tensors with the one computed in previous steps\n",
    "            all_pred = torch.cat((all_pred,pred))          \n",
    "            all_targ = torch.cat((all_targ,target_labels.long().cpu()))\n",
    "            all_ids = torch.cat((all_ids,target_ids))\n",
    "\n",
    "            batch_loss += loss.item()   #sum loss \n",
    "            \n",
    "\n",
    "    acc, f1 = acc_and_f1(all_targ,all_pred)\n",
    "\n",
    "    maj_acc, maj_f1 = acc_and_f1_majority(all_targ,all_pred,all_ids)\n",
    "\n",
    "    loss = batch_loss/(batch_id+1)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    log.debug('eval epoch time:',start-end)\n",
    "\n",
    "    return loss, acc, f1, maj_acc, maj_f1, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model: Custom_model, dataloaders: tuple[BucketIterator,...], param : dict(), device):\n",
    "    \"\"\"\n",
    "        Runs the train and eval loop and keeps track of all the metrics of the training model \n",
    "    \"\"\"\n",
    "    model_metrics = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"train_maj_acc\": [],\n",
    "        \"train_maj_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_f1\": [],\n",
    "        \"val_maj_acc\": [],\n",
    "        \"val_maj_f1\": [],\n",
    "        \"train_predictions\": [],\n",
    "        \"eval_predictions\": []\n",
    "    }\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=param['weight_positive_class']).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=param['lr'],  weight_decay=param['weight_decay'])   #L2 regularization \n",
    "\n",
    "    train_dataloader, eval_dataloader = dataloaders\n",
    "\n",
    "    for epoch in range(param['n_epochs']): #epoch loop\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        train_metrics = train_loop(model, train_dataloader, optimizer, criterion, device) #train\n",
    "        eval_metrics = eval_loop(model, eval_dataloader, criterion, device) #eval\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        tot_epoch_time = end_time-start_time          \n",
    "\n",
    "        train_epoch_loss, train_epoch_acc, train_epoch_f1, train_epoch_maj_acc, train_epoch_maj_f1,train_predictions = train_metrics\n",
    "        eval_epoch_loss, eval_epoch_acc, eval_epoch_f1, eval_epoch_maj_acc, eval_epoch_maj_f1, eval_predictions = eval_metrics\n",
    "\n",
    "\n",
    "        #log Train and Validation metrics\n",
    "        model_metrics['train_loss'].append(train_epoch_loss)\n",
    "        model_metrics['train_acc'].append(train_epoch_acc)\n",
    "        model_metrics['train_f1'].append(train_epoch_f1)\n",
    "        model_metrics['train_maj_acc'].append(train_epoch_maj_acc)\n",
    "        model_metrics['train_maj_f1'].append(train_epoch_maj_f1)\n",
    "        model_metrics['val_loss'].append(eval_epoch_loss)\n",
    "        model_metrics['val_acc'].append(eval_epoch_acc)\n",
    "        model_metrics['val_f1'].append(eval_epoch_f1)\n",
    "        model_metrics['val_maj_acc'].append(eval_epoch_maj_acc)\n",
    "        model_metrics['val_maj_f1'].append(eval_epoch_maj_f1)\n",
    "        model_metrics['train_predictions'].append(train_predictions)\n",
    "        model_metrics['eval_predictions'].append(eval_predictions)\n",
    "        \n",
    "\n",
    "        \n",
    "        log.debug(f'epoch {epoch+1} time {tot_epoch_time}')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {tot_epoch_time:.4f}')\n",
    "        print(f'\\tTrain Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f} | Train Maj Acc: {train_epoch_maj_acc*100:.2f}%  | Train Maj F1: {train_epoch_maj_f1:.2f}%  | Train Predictions: {train_predictions}')\n",
    "        print(f'\\t Val. Loss: {eval_epoch_loss:.3f} | Val. Acc: {eval_epoch_acc*100:.2f}% | Val. F1: {eval_epoch_f1:.2f}  | Val Maj Acc: {train_epoch_maj_acc*100:.2f}%  | Val Maj F1: {train_epoch_maj_f1:.2f}%  | Val Predictions: {eval_predictions}')\n",
    "    \n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the models we have to define some  useful parameters and hyperparameters to pass to the train and validation loops. According to our experimental results obtained after a tuning phase, these are the ones that we are using to train each architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS, HYPERPARAMETERS AND USEFUL OBJECTS \n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'running on {DEVICE}')\n",
    "\n",
    "#param\n",
    "PAD_IDX = 0                     # pad index\n",
    "\n",
    "#hyperparameters\n",
    "BATCH_SIZE = 128                # number of sentences in each mini-batch\n",
    "LR = 1e-3                       # learning rate \n",
    "N_EPOCHS = 15                   # number of epochs\n",
    "WEIGHT_DECAY = 1e-5             # regularization\n",
    "\n",
    "#to counteract class imbalance \n",
    "(supports, refutes) = df.loc[df['split'] == 'train']['idx_label'].value_counts()    #number of supports and refutes in the train dataset \n",
    "weight_positive_class = torch.Tensor([refutes/supports]).to(DEVICE)  #weight to give to positive class \n",
    "\n",
    "#parameters dictionary \n",
    "param = {\n",
    "    'lr':LR,\n",
    "    'n_epochs':N_EPOCHS,\n",
    "    'weight_decay':WEIGHT_DECAY,\n",
    "    'weight_positive_class': weight_positive_class\n",
    "    }\n",
    "\n",
    "\n",
    "max_tokens = max(df.claim.apply(len).max(),df.evidence.apply(len).max())  #max number of tokens in a sentence in the entire dataset \n",
    "\n",
    "#create dataloaders \n",
    "train_dataloader,val_dataloader,test_dataloader = create_dataloaders(BATCH_SIZE,df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUZIONE A TRAINARE TUTTI I MODELLI PRIMA CON UNA STRATEGIA DI SENTENCE EMB E CON LA MIGLIORE TUTTE LE MERGE INPUT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIPO DI MODELLO CHE STIAMO PER FARE (PRIMA DI OGNI MODELLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = Architecture('mlp','concat',True)                    \n",
    "model = Custom_model(embedding_matrix, PAD_IDX, max_tokens, architecture, DEVICE)\n",
    "\n",
    "model_metrics = train_and_eval(model, (train_dataloader,val_dataloader),param,DEVICE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16b10856870a6af5fec4ffddd4d7318a6f2add2c9f3b4bd7caecf75cea33b7bd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
